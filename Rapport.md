# Reducing the Dimensionality of Data with Neural Networks

by G. E. Hinton and R. R. Salakhutdinov

## Abstract

Des donn√©es en haute dimension peuvent √™tre un probl√®me et n√©cessitent parfois d'√™tre repr√©sent√©es avec moins de dimensions pour des applications multiples comme la visualisation, l'acc√©l√©ration algorithmique ou encore le stockage et la compression de donn√©es. 

Une m√©thode classique et largement utilis√©e dans ce domaine est l'analyse en composantes principales et nous allons expliquer comment l'appliquer.

Nous allons ensuite explorer une m√©thode de r√©duction de dimension bas√©e sur une architecture de neurone multicouche "auto-codeurs". Des algorithmes de descente de gradient sont g√©n√©ralement utilis√©s sur ces architectures mais cela ne fonctionne bien que si les poids initiaux sont proches d'une bonne solution.

Nous allons expliquer et impl√©menter une solution propos√©e par [G. E. Hinton et R. R. Salakhutdinov](www.cs.toronto.edu/~hinton/science.pdf) qui permet d'initialiser efficacement les poids du r√©seau d'auto-encodage profond √† l'aide de plusieurs couches cons√©cutives sur le mod√®le des *Restricted Boltzmann Machines*. Le r√©seaux pourra ensuite affiner ses poids √† l'aide d'un algorithme de descente de gradient afin d'apprendre des codes de faible dimension qui fonctionnent beaucoup mieux qu'un r√©seau d'auto-encodage na√Øf.

Enfin, nous allons comparer les r√©sultats obtenus et conclure sur quelle est la plus efficace des solutions pr√©sent√©es.

<div style="page-break-after: always; break-after: page;"></div>

[TOC]

<div style="page-break-after: always; break-after: page;"></div>

## Introduction

- Presentation of the problem(s). 

- Previous works (at least a few citations). If relevant, include things that you have seen during the lectures.

- Contributions. Why is the studied method different/better/worse/etc. than existing previous works. 


## Analyse en composantes principales

### L'id√©e

L‚Äôanalyse en composantes principales (ACP) est une technique tr√®s populaire.

En termes simples, l'ACP consiste √† effectuer une transformation de coordonn√©es √† partir des axes arbitraires avec lesquels nos donn√©es sont exprim√©es vers un ensemble d'axes "align√©s avec les donn√©es elles-m√™mes". C'est √† dire, des axes qui expriment au mieux la dispersion des informations pr√©sentes. La dispersion n'est rien d'autre que la variance ou le fait de disposer d'une "information √©lev√©e". On peut dire en d'autres termes qu'une dispersion √©lev√©e contient une information √©lev√©e.

Par cons√©quent, si nos donn√©es sont exprim√©es par des axes maximisant la repr√©sentation de l'information, il est alors possible d'omettre les axes ou les dimensions ayant une variance moindre car ces "composantes" ne participent que tr√®s peu √† la description des donn√©es.
Cela peut par exemple servir √† am√©liorer la rapidit√© de certains algorithmes en √©pargnant de nombreux calculs sans trop souffrir de perte de pr√©cision.

Selon les points de vue, on peut consid√©rer cette analyse comme une technique descriptive o√π l‚Äôon essaie de r√©sumer les donn√©es selon ses dimensions les plus importantes. Cela peut √™tre utilis√© comme une technique de visualisation o√π l‚Äôon essaie de pr√©server la "proximit√©" entre les individus dans un espace de repr√©sentation r√©duit par exemple.

Nous d√©finirons les termes suivants au fur et √† mesure, mais voici le processus r√©sum√© :

- Trouver la matrice de covariance pour l'ensemble de donn√©es en question
- Trouver les vecteurs propres de cette matrice
- Trier les vecteurs propres/"dimensions" de la plus grande √† la plus petite variance
- Projection / R√©duction des donn√©es : Utiliser les vecteurs propres correspondant √† la plus grande variance pour projeter l'ensemble de donn√©es dans un espace √† dimensions r√©duites
- V√©rification : combien avons-nous perdu en pr√©cision dans la repr√©sentation des donn√©es ?

### Variance et covariance

Techniquement, la variance est la moyenne des diff√©rences au carr√© par rapport √† la moyenne. Si vous connaissez bien l'√©cart type, g√©n√©ralement not√© $\sigma$, la variance est juste le carr√© de l'√©cart type. 
$$
V = \sigma^2 = {1\over N}\sum_{i=0}^N (x_i - \bar{x})^2
$$
Avec la moyenne not√©e $\bar{x}$:
$$
\bar{x} = {1\over N}\sum_{i=0}^N x_i
$$
La variance exprime la "propagation" ou "l'√©tendue" des donn√©es.

Exemple d'un jeu de donn√©es 2D :

![plot2D](./src/PCA/imgs/plot2D.png)

Il est possible de calculer la variance selon les deux axes : 

| axe      | x                  | y                   |
| -------- | ------------------ | ------------------- |
| variance | 1.2526627262767291 | 0.31870756461533817 |

On peut remarquer dans le graphique ci-dessus que $x$ varie "avec" $y$ √† peu pr√®s. On dit alors que $y$ est "covariant" avec $x$ . 

La covariance indique le niveau auquel deux variables varient ensemble.
Pour la calculer, c'est un peu comme la variance r√©guli√®re, sauf qu'au lieu d'√©lever au carr√© l'√©cart par rapport √† la moyenne pour une variable, nous multiplions les √©carts pour les deux variables.
$$
Cov(x,y) = {1\over N-1}\sum_{j=1}^N (x_j-\bar x)(y_j-\bar y)
$$

> Remarque : la covariance d'une variable avec elle m√™me est sa variance.

>  <span style="color:red">Remarque :</span> On divise ici par $N-1$ au lieu de $N$, donc contrairement √† la variance r√©guli√®re, nous ne prenons pas tout √† fait la moyenne. Pour un grand ensemble de donn√©es cela ne fait essentiellement aucune diff√©rence, mais pour un petit nombre de points de donn√©es, l'utilisation de ùëÅ peut donner des valeurs qui ont tendance √† √™tre trop petites et donc le $N-1$ a √©t√© introduit pour "r√©duire le biais des petits √©chantillons".

La covariance de $x$ en fonction de $y$ vaut donc dans notre exemple $0.6250769297631616$

### Matrice de covariance

La matrice de covariance est une matrice qui regroupe la variance et covariance de chaque variable ( ou dimension) avec chaque autres et s'exprime sous la forme :
$$
\begin{pmatrix}
   Cov(x, x) & Cov(x, y) \\
   Cov(x, y) & Cov(y, y) \\
\end{pmatrix}
$$
Le long de la diagonale se trouvera la variance de chaque variable (√† ${1 \over N}$ pr√®s), et le reste de la matrice sera constitu√©e des covariances. 

> Remarque : Puisque l'ordre des variables n'a pas d'importance lors du calcul de la covariance, la matrice sera *sym√©trique* et sera donc *carr√©e*.

On obtient dans notre exemple : 
$$
\begin{pmatrix}
1.25895751 & 0.62507693 \\
0.62507693 & 0.32030911 \\
\end{pmatrix}
$$
Nous avons donc maintenant une matrice de covariance. L'√©tape suivante de l'ACP consiste √† trouver les "composantes principales". Cela signifie les directions dans lesquelles les donn√©es varient le plus. Cela n√©cessite de trouver des vecteurs propres pour la matrice de covariance de notre ensemble de donn√©es.

### Vecteurs propres

Par d√©finition, √©tant donn√© une matrice (ou "op√©rateur lin√©aire")  ${\bf A}$ de taille $n\times n$ il existe un ensemble de $n$ vecteurs $\vec{v}_i$ de sorte que la multiplication d'un de ces vecteurs par ${\bf A}$ donne un vecteur proportionnel (d'un facteur $\lambda_i$) √† $\vec{v}_i$.
$$
{\bf A} \vec{v}_i = \lambda_i \vec{v}_i
$$
On appelle ces vecteurs les vecteurs propres et les $\lambda_i$ leurs valeurs propres associ√©es.

Nous ne rentrerons pas dans le d√©tails de comment obtenir ces vecteurs et valeurs propres ici car de nombreuses librairies permettent de le faire et beaucoup mieux que nous.

Il s'agit en g√©n√©ral pour les cas les plus simples de suivre les √©tapes suivantes: 

- Trouver les valeurs propres (en r√©solvant le syst√®me $det( \bf{A} - \lambda \bf{I}) = 0$ )
- Pour chaque valeur propre, obtenir un syst√®me d'√©quations lin√©aires pour chaque vecteur propre et les r√©soudre


On obtient pour notre jeu de donn√©es :

$\lambda_1 = 1.57128949$ et $\lambda_2 = 0.00797714$

$v_1 = \begin{pmatrix}0.89454536 \\ 0.44697717\end{pmatrix}$ et $v_2 = \begin{pmatrix}-0.44697717 \\ 0.89454536\end{pmatrix}$

![plot2DEigensVectors](src\PCA\imgs\plot2DEigensVectors.png)

On remarque bien ici que le vecteur propre $v_1$ indique la direction qui maximise la variance de nos donn√©e. On a donc bien trouv√© ici l'axe "principal" de notre jeu de donn√©es. Le deuxi√®me vecteur propre pointe dans la direction de la plus petite variance et est orthogonal au premier vecteur.

>  Remarque : la longueur des vecteurs est exprim√©e √† l'aide de leurs valeurs propre pour illustrer l'importance des diff√©rents axes dans la variance de nos donn√©es.

### Projection des donn√©es

Nous avons maintenant nos composantes (vecteurs propres), et nous les avons "class√©es" selon leur "importance". Nous allons maintenant √©liminer les directions de faible variance moins importantes. En d'autres termes, nous allons projeter les donn√©es sur les diff√©rentes composantes principales de plus grande variance.

Il est possible par un changement de base d'exprimer nos donn√©es selon ces axes principaux :

![projectedData](src\PCA\imgs\EigenBaseData.png)

La matrice de covariance obtenue avec ces donn√©es exprim√©es dans notre nouvelle base donne : 
$$
\approx
\begin{pmatrix}
	1.57128949 & 0 \\
	0 & 0.00797714 \\
\end{pmatrix}
$$
Ce nouveau "syst√®me de coordonn√©es" exprime les donn√©es dans des "directions" d√©coupl√©es les unes des autres ($Cov(a_i,a_j) = 0 \quad\forall i \neq j$)

C'est pour cette raison que les vecteurs propres de nos donn√©es sont int√©ressants.

Intuitivement on se doutait bien depuis le d√©but qu'il serait int√©ressant d'exprimer nos donn√©es selon un unique axe, puisque elles s'apparentent grossi√®rement √† une droite.

---

Pour effectuer la projection il va nous suffire de r√©duire √† z√©ro la dimension de plus petite variance, ce qui reviens √† effectu√© un "changement de base" avec seulement les vecteurs de plus grande variance (dans notre cas, un seul vecteur).

On obtient alors les donn√©es suivantes : 

![projectedData](src\PCA\imgs\projectedData.png)

On peut ramener ensuite nos donn√©es dans notre base d'origine pour comparer avec nos donn√©es d'origine. Cette √©tape est utilis√©e uniquement afin de comparer notre projection dans le m√™me "syst√®me de coordonn√©es" que nos donn√©es d'origine ou autrement dit dans la m√™me dimension dans le sens o√π il s'agit ici d'ajouter une dimension "superflue" √† nos donn√©es ainsi r√©duite.

![projectedData](src\PCA\imgs\backProjectedData.png)

### Le cas r√©el en hautes dimensions

L'ACP est g√©n√©ralement utilis√©e pour √©liminer beaucoup plus de dimensions qu'une seule (comme dans notre exemple 2D). Elle est souvent utilis√©e pour la visualisation des donn√©es, mais aussi pour la r√©duction des caract√©ristiques, c'est-√†-dire pour envoyer moins de donn√©es dans votre algorithme d'apprentissage afin d'am√©liorer ses performances.

Dans notre cas 2D il √©tait √©vident qu'une seule direction ou dimension √©tait int√©ressante pour exprimer nos donn√©es ; mais dans les cas de grande dimension, comment savoir combien de dimensions omettre ? En d'autres termes, combien de "composants" doit-on conserver lors de l'ACP ?

Il y a plusieurs fa√ßons de faire ce choix. Il faudra g√©n√©ralement faire un compromis entre la pr√©cision et la vitesse de calcul. On peut par exemple exprimer sur un graphique la variance des donn√©es en fonction du nombre de composantes que l'on garde.

Abordons pour finir cette partie sur l'ACP un exemple plus concret illustrant cette m√©thode de s√©lection et un cas "r√©el" d'application de l'ACP.

### Example: MNIST dataset

Prenons un ensemble d'images de 28x28 pixels repr√©sentant des chiffres manuscrits et appliquons l'analyse en composantes principales sur ces images consid√©r√©es comme des vecteurs de 784 composantes (dans un espace de 784 dimensions donc).

![MnistExamples](src\PCA\imgs\MnistExamples.png)

Apr√®s calcul des vecteurs et valeurs propres de nos donn√©es, tra√ßons le graphe des valeurs propres rang√©es par ordre d√©croissant :

![eigensValuesByComponents](src\PCA\imgs\eigensValuesByComponents.png)

On aper√ßoit ici que les valeurs des valeurs propres d√©croissent tr√®s rapidement. On peut interpr√©ter cela comme une d√©croissance tr√®s rapide de la variances de nos donn√©es expliqu√©es par les composantes ; ou autrement dit, "peu" de composantes repr√©sentent une partie significative de la variance de nos donn√©es. 

> Rappel : plus la valeur propre d'un vecteur propre est grande plus la variance expliqu√©e par ce vecteur est grande

Exprimons maintenant ce m√™me graphique en valeurs cumul√©es et en normalisant les valeurs propres.

![cumulativeExplainedVariance](src\PCA\imgs\cumulativeExplainedVariance.png)

On obtient alors le  graphe de la variance cumulative expliqu√©e en fonction du nombre de composantes.
Ce graphe permet alors de conna√Ætre pour un nombre de composantes donn√© le pourcentage de variance des donn√©es expliqu√©, ce qui peut s'interpr√©ter comme un pourcentage de pr√©cision √† repr√©senter les donn√©es d'origine avec ce nombre de composantes.
On voit par exemple qu'avec seulement 330 composantes il est possible de repr√©senter √† 99% nos donn√©es d'origine.

---

Pour conclure avec cet exemple, voici un tableau de diff√©rents chiffres projet√©s avec diff√©rents niveaux de variance expliqu√©e (et le nombre de composantes retenues).
Les chiffres sous les images correspondent √† trois mesures permettant l'√©valuation de la qualit√© de projection des images ; √† savoir de haut en bas : le rapport signa-bruit (PSNR), l'erreur quadratique moyenne (MSE) et la structural similarity (SSIM).

![MnistEvaluation](src\PCA\imgs\MnistEvaluation.png)

## Restricted Boltzmann Machines

Invent√©es par Geoffrey Hinton, les machines de Boltzmann restreintes sont des r√©seaux neuronaux peu profonds √† deux couches qui constituent les √©l√©ments de base des r√©seaux profonds.

### L'id√©e

Une RBM est utilis√©e pour avoir une estimation de la distribution probabiliste d'un jeu de donn√©es.

La premi√®re couche de la RBM est appel√©e la couche visible, ou couche d'entr√©e, et la seconde est la couche cach√©e.

### √ânergie d'activation

On d√©finit l'√©nergie d'activation d'une machine de Boltzmann restreinte par la formule suivante :
$$
E = -\sum_i b_iv_i - \sum_j c_jh_j - \sum_{i,j} w_{ij}v_ih_j
$$
avec :

- $w_{ij}$ le poids entre le neurone $j$ et le neurone $i$
- $v_i$ l'√©tat du neurone $i$ de la couche visible
- $h_j$ l'√©tat du neurone $j$ de la couche cach√©e (hidden)
- $b_i$ et $c_j$ les biais des neurones $v_i$ et $h_j$ d'entr√©e et de sortie

Cette √©nergie est interpr√©t√©e comme l'√©nergie d'un syst√®me physique et on peut d√©finir le score d'une configuration √©nerg√©tique comme l'inverse de cette √©nergie. Plus l'√©nergie est faible (stabilit√©) plus le score est √©lev√©.
$$
\text{Score} = - E
$$

### Probabilit√©

Consid√©rons des scores donn√©s pour des configurations possibles de notre syst√®me.
$$
[ 0, 1, 2, 5, 8]
$$
Il est int√©ressant d'exprimer le score de ces configurations en probabilit√©s et le moyen le plus classique est de normaliser ces scores par la somme de tous les scores :

![scoresProba](src\RBM\imgs\scoresProba.png)

Cependant des probl√®mes peuvent survenir avec des scores n√©gatifs car ils peuvent se compenser et la somme peut alors √™tre nulle.

Un moyen naturel peut √™tre de passer √† l'exponentielle puis de normaliser, c'est ce qu'on appelle l'op√©ration de softmax.

![softmax](src\RBM\imgs\softmax.png)

C'est ainsi que l'on peut d√©finir la probabilit√© d'avoir une certaine configuration entr√©e-sortie $(v_i, h_j)$ :
$$
P(v_i, h_j) = {e^{-E(v_i, h_j)} \over Z}
$$
o√π $Z$ est la constante de normalisation.

### Positionnement du probl√®me

L'id√©e g√©n√©rale est de modifier les poids $w_{ij}$ pour approcher au mieux la distribution de probabilit√© de nos donn√©es.
C'est diff√©rent d'un algorithme plus classique comme une r√©gression par exemple, qui estime une valeur continue bas√©e sur de nombreuses entr√©es.

Imaginons que les donn√©es d'entr√©e et les reconstructions soient des courbes normales de formes diff√©rentes, qui ne se chevauchent que partiellement.

En ajustant it√©rativement les poids en fonction de l'erreur qu'ils produisent ou de leurs scores, une RBM apprend √† se rapprocher des donn√©es originales en mimant la distribution de probabilit√© des donn√©es d'origine dans les donn√©es de la couche cach√©e. On pourrait dire que les poids en viennent lentement √† refl√©ter la structure de l'entr√©e qui est encod√©e dans la couche cach√©e.

Consid√©rons un exemple simple dans lequel une personne dispose de trois accessoires :

Une paire de lunette (not√©e **L**), un parapluie (not√© **P**) et une cam√©ra (not√©e **C**)

Regardons ce qu'elle d√©cide ou non de prendre lorsqu'elle sort de chez elle en fonction des jours de la semaine :

| jour | Lunettes :eyeglasses: | Parapluie :closed_umbrella: | Cam√©ra :camera:    |
| ---- | --------------------- | --------------------------- | ------------------ |
| 0    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 1    | :x:                   | :heavy_check_mark:          | :x:                |
| 2    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 3    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 4    | :x:                   | :heavy_check_mark:          | :x:                |
| 5    | :x:                   | :heavy_check_mark:          | :x:                |
| 6    | :x:                   | :heavy_check_mark:          | :heavy_check_mark: |
| 7    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 8    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 9    | :x:                   | :heavy_check_mark:          | :x:                |

Ces donn√©es vont constituer nos donn√©es d'entr√©e de la couche d'input.

Consid√©rons maintenant qu'il existe deux facteurs qui pourraient expliquer ces donn√©es : la pr√©sence ou non de soleil :high_brightness: (not√© **S**) et d'averse :sweat_drops: ‚Äã(not√© **A**) au cours de la journ√©e. ‚Äã

Ces facteurs vont constituer la couche cach√©e de notre syst√®me.

Initialisons maintenant tous nos poids √† 0 et tra√ßons la probabilit√© de chaque configuration d'entr√©e-sortie.

> Nous ignorerons les biais associ√©s aux entr√©es et sorties dans notre exemple pour plus de simplicit√©.

> Une configuration est not√©e avec une suite de lettres montrant la pr√©sence ou non de l'accessoire et d'un √©v√®nement m√©t√©orologique.

![uniformProbabilities](src\RBM\imgs\uniformProbabilities.png)

On constate √©videment que toutes les configurations sont √©quiprobables (car tous nos poids sont nuls et donc l'√©nergie de chaque configuration est nulle)

On aimerait se rapprocher d'une configuration de probabilit√©s qui repr√©sente nos donn√©es, c'est-√†-dire qui exprime les formations possibles de notre jeu de donn√©es.

Dans notre cas voici les configurations qui apparaissent tout au long de la semaine concernant les inputs : 

| jour          | 0                     | 1    | 2                     | 3                     | 4                 | 5                 | 6                         | 7                     | 8                     | 9                 |
| ------------- | --------------------- | ---- | --------------------- | --------------------- | ----------------- | ----------------- | ------------------------- | --------------------- | --------------------- | ----------------- |
| configuration | :eyeglasses: :camera: | :    | :eyeglasses: :camera: | :eyeglasses: :camera: | :closed_umbrella: | :closed_umbrella: | :closed_umbrella::camera: | :eyeglasses: :camera: | :eyeglasses: :camera: | :closed_umbrella: |

En incluant les configurations possibles de notre couche cach√©e, il est possible de lister ainsi toutes les configurations (entr√©e et sortie) envisageables:

- :eyeglasses: :camera:
- :eyeglasses: :camera::sweat_drops:
- :eyeglasses: :camera::high_brightness:
-  : ::: 
- :closed_umbrella:
- :closed_umbrella::sweat_drops:
- :closed_umbrella::high_brightness:
-  :closed_umbrella::high_brightness::sweat_drops: 
- :closed_umbrella: :camera:
- :closed_umbrella: :camera::sweat_drops:
- :closed_umbrella: :camera::high_brightness:
- :closed_umbrella: :camera::high_brightness::sweat_drops: 

On aimerait donc trouver les poids de notre algorithme pour que les configurations ou √©v√®nements envisageables aient une grande probabilit√© et les autres une faible probabilit√©.

Cela donnerait quelque chose comme cela pour notre exemple :

![exempleWantedProbabilities](src\RBM\imgs\exempleWantedProbabilities.png)

> L'√©v√®nement :eyeglasses: :camera: (LC) a lieu 5 fois dans la semaine. Il est donc normal que cette configuration soit plus probable que l'√©v√®nement :closed_umbrella: (P) (qui a lieu 3 fois) ou que l'√©v√®nement :closed_umbrella: :camera: (PC) (qui a lieu une unique fois).

On peut reformuler cela math√©matiquement dans le sens o√π on cherche √† maximiser le produit des probabilit√©s des √©v√®nements probables de notre jeu de donn√©e.
$$
arg \; \underset{W}{max}\;\underset{v \in V}{\Pi}P(v)
$$


### Apprentissage

####  *Contrastive Divergence*



## Solution propos√©e

TODO 

ajout des photo tir√©es du papier, unfolding multicouche, ..



## Comparaison



## Conclusion

> Learning with examples first is always better than starting with math

r√©sultats obtenus, ..

limitations, erreurs, ouvertures, am√©liorations, papiers plus r√©cents sur la m√™me probl√©matique



Bien que les RBM soient parfois utilis√©s, ils sont peu √† peu ont d√©pr√©ci√©s au profit de r√©seaux adversaires g√©n√©rateurs (GAN) ou d'auto-codeurs variationnels(VAE) .

## Bibliographie

Vecteur propres : 

[3Blue1Brown]: https://www.youtube.com/watch?v=PFDu9oVAE-g	"Les vecteurs propres et valeurs propres"


# Reducing the Dimensionality of Data with Neural Networks

by G. E. Hinton and R. R. Salakhutdinov

## Abstract

Des donn√©es en haute dimension peuvent √™tre un probl√®mes et n√©cessite parfois d'√™tre repr√©sent√©es avec moins de dimensions pour des applications multiples comme la visualisation, acc√©l√©ration algorithmique  ou encore le stockage et la compression de donn√©es. 

Une m√©thode classique et largement utilis√©e dans ce domaine √† savoir est l'analyse en composantes principales et nous allons expliquer comment l'appliquer.

Nous allons ensuite explorer une m√©thode de r√©duction de dimension bas√© sur une architecture de neurone multicouche "auto-codeurs". Des algorithmes de descente de gradient sont g√©n√©ralement utilis√©es sur ces architectures mais cela ne fonctionne bien que si les poids initiaux sont proches d'une bonne solution.

Nous allons expliquer et impl√©menter une solution propos√©e par [G. E. Hinton et R. R. Salakhutdinov](www.cs.toronto.edu/~hinton/science.pdf)  qui permet d'initialiser efficacement les poids du r√©seaux d'auto-codage profonds √† l'aide de plusieurs couches cons√©cutives sur le mod√®le des *Restricted Boltzmann Machines*.  Le r√©seaux pourra ensuite   affiner ses poids √† l'aide d'un algorithme de descente de gradient afin d'apprendre des codes de faible dimension qui fonctionnent beaucoup qu'un r√©seau d'auto-codage na√Øf.

Enfin, nous allons comparer les r√©sultats obtenues et conclure sur la solution la plus efficace des solutions pr√©sent√©es.

<div style="page-break-after: always; break-after: page;"></div>

[TOC]

<div style="page-break-after: always; break-after: page;"></div>

## Introduction

- Presentation of the problem(s). 

- Previous works (at least a few citations). If relevant, include things that you have seen during the lectures.

- Contributions. Why is the studied method different/better/worse/etc. than existing previous works. 

  

## Analyse en composantes principales

### L'id√©e

L‚Äôanalyse en composantes principales (ACP) est une technique tr√®s populaire.

En termes simples, l'ACP consiste √† effectuer une transformation de coordonn√©es √† partir des axes arbitraires avec lesquels nos donn√©es sont exprim√©es vers un ensemble d'axes "align√©s avec les donn√©es elles-m√™mes". C'est √† dire, des axes qui expriment au mieux la dispersion des informations pr√©sentes. La dispersion n'est rien d'autre que la variance ou le fait de disposer d'une "information √©lev√©e". On peut dire en d'autres termes qu'une dispersion √©lev√©e contient une information √©lev√©e.

Par cons√©quent, si nos donn√©es sont exprim√©es par des axes maximisant la repr√©sentation de l'information, il est alors possible d'omettre les axes ou les dimensions ayant une variance moindre car ces "composantes" ayant une faible variance ne participent que tr√®s peu √† la description des donn√©es. 
Cela peut par exemple servir √† am√©liorer la rapidit√© de certains algorithme en √©pargnant de nombreux calculs sans trop souffrir de perte de pr√©cision.

Selon les points de vue, on peut consid√©rer cet analyse comme une technique descriptive o√π l‚Äôon essaie de r√©sumer les donn√©es dans ses dimensions les plus importantes. Cela peut √™tre utilis√© comme une technique de visualisation o√π l‚Äôon essaie de pr√©server la "proximit√©" entre les individus dans un espace de repr√©sentation r√©duit par exemple.

Nous d√©finirons les termes suivants au fur et √† mesure, mais voici le processus r√©sum√© :

- Trouvez la matrice de covariance pour votre ensemble de donn√©es

- Trouvez les vecteurs propres de cette matrice
- Trier les vecteurs propres/"dimensions" de la plus grande √† la plus petite variance
- Projection / R√©duction des donn√©es : Utiliser les vecteurs propres correspondant √† la plus grande variance pour projeter l'ensemble de donn√©es dans un espace √† dimensions r√©duites
- V√©rification : combien avons-nous perdu en pr√©cision dans la repr√©sentation des donn√©es  ?

### Variance et covariance

Techniquement, la variance est la moyenne des diff√©rences au carr√© par rapport √† la moyenne. Si vous connaissez bien l'√©cart type, g√©n√©ralement d√©sign√© via l'√©cart type not√© $\sigma$, la variance est juste le carr√© de l'√©cart type. 
$$
V = \sigma^2 = {1\over N}\sum_{i=0}^N (x_i - \bar{x})^2
$$
Avec la moyenne not√©e $\bar{x}$:
$$
\bar{x} = {1\over N}\sum_{i=0}^N x_i
$$
La variance exprime la "propagation" ou "l'√©tendue" des donn√©es.

Exemple d'un jeu de donn√©es 2D :

![plot2D](./src/PCA/imgs/plot2D.png)

Il est possible de calculer la variance selon les deux axes : 

| axes     | x                  | y                   |
| -------- | ------------------ | ------------------- |
| variance | 1.2526627262767291 | 0.31870756461533817 |

On peut remarquerez dans le graphique ci-dessus que $x$ varie "avec" $y$ √† peu pr√®s. On dit alors que $y$ est "covariant" avec $x$ . 

La covariance indique le niveau auquel deux variables varient ensemble.
Pour la calculer, c'est un peu comme la variance r√©guli√®re, sauf qu'au lieu d'√©lever au carr√© l'√©cart par rapport √† la moyenne pour une variable, nous multiplions les √©carts pour les deux variables.
$$
Cov(x,y) = {1\over N-1}\sum_{j=1}^N (x_j-\bar x)(y_j-\bar y)
$$

> Remarque la covariance d'une variable avec elle m√™me est sa variance.

>  <span style="color:red">Remarque :</span> On divise ici par $N-1$ au lieu de $N$, donc contrairement √† la variance r√©guli√®re, nous ne prenons pas tout √† fait la moyenne. Pour un grands ensembles de donn√©es cela ne fait essentiellement aucune diff√©rence, mais pour un petit nombre de points de donn√©es, l'utilisation de ùëÅ peut donner des valeurs qui ont tendance √† √™tre trop petites et donc le $N-1$ a √©t√© introduit pour "r√©duire le biais des petits √©chantillons".

la covariance de $x$ en fonction de $y$ faut donc dans notre exemple $0.6250769297631616$

### Matrice de covariance

La matrice de covariance est une matrice qui regroupe la variance et covariance de chaque variables ( ou dimensions) avec chaque autres  et s'exprime sous la forme :
$$
\begin{pmatrix}
   Cov(x, x) & Cov(x, y) \\
   Cov(x, y) & Cov(y, y) \\
\end{pmatrix}
$$
Le long de la diagonale se trouvera la variance de chaque variable (√† ${1 \over N}$ pr√®s), et le reste de la matrice sera constitu√©e des covariances. 

> Remarque : Puisque l'ordre des variables n'a pas d'importance lors du calcul de la covariance, la matrice sera *sym√©trique* et sera donc *carr√©e*.

On obtient dans notre exemple : 
$$
\begin{pmatrix}
1.25895751 & 0.62507693 \\
0.62507693 & 0.32030911 \\
\end{pmatrix}
$$
Nous avons donc maintenant une matrice de covariance. L'√©tape suivante de l'ACP consiste √† trouver les "composantes principales". Cela signifie les directions dans lesquelles les donn√©es varient le plus. Cela n√©cessite de trouver des vecteurs propres pour la matrice de covariance de notre ensemble de donn√©es.

### Vecteurs propres

Par d√©finition, √âtant donn√© une matrice (ou "op√©rateur lin√©aire")  ${\bf A}$ de dimensions $n\times n$ il existe un ensemble de $n$ vecteurs $\vec{v}_i$ de sorte que la multiplication d'un de ces vecteurs par ${\bf A}$ donne un vecteur proportionnel(d'un facteur $\lambda_i$) √† $\vec{v}_i$.
$$
{\bf A} \vec{v}_i = \lambda_i \vec{v}_i
$$
On appel ces vecteurs les vecteurs propres et les $\lambda_i$ leurs valeurs propres.

Nous ne rentrerons pas dans le d√©tails de comment obtenir ces vecteurs et valeurs propre ici car de nombreuses librairies permettre de le faire et beaucoup mieux que nous.

Il s'agit en g√©n√©ral pour les cas les plus simples de suivre les √©tapes suivantes: 

- Trouver les valeurs propres (en r√©solvant la formule $det( \bf{A} - \lambda I) = 0$ )
- Pour chaque valeur propre, obtenir un syst√®me d'√©quations lin√©aires pour chaque vecteur propre et les r√©soudre



On obtient pour notre jeu de donn√©es :

$\lambda_1 = 1.57128949$ et $\lambda_2 = 0.00797714$

$v_1 = \begin{pmatrix}0.89454536 \\ 0.44697717\end{pmatrix}$ et $v_2 = \begin{pmatrix}-0.44697717 \\ 0.89454536\end{pmatrix}$

![plot2DEigensVectors](src\PCA\imgs\plot2DEigensVectors.png)

On remarque bien ici que le vecteur propre $v_1$ indique la direction qui maximise la variance de nos donn√©e. On √† donc bien trouv√© ici l'axe "principale" de notre jeu de donn√©e. Le deuxi√®me vecteur propre pointe dans la direction de la plus petite variance et est orthogonale au premier vecteur.

>  Remarque : la longueur des vecteurs est exprim√©e √† l'aide de leurs valeurs propre pour illustrer l'importance des diff√©rents axes dans la variance de nos donn√©es.

### Projection des donn√©es

Nous avons maintenant nos composants (vecteurs propres), et nous les avons "class√©s" selon leur "importance". Nous allons maintenant √©liminer les directions de faible variance moins importantes. En d'autres termes, nous allons projeter les donn√©es sur les diff√©rentes composantes principales de plus grande variance.

Il est possible par un changement de base d'exprimer nos donn√©es selon ces axes principaux :

![projectedData](src\PCA\imgs\EigenBaseData.png)

La matrice de covariance obtenue avec ces donn√©es exprim√©s dans notre nouvelle base donne : 
$$
\approx
\begin{pmatrix}
	1.57128949 & 0 \\
	0 & 0.00797714 \\
\end{pmatrix}
$$
Ce nouveau "syst√®me de coordonn√©es" exprimes les donn√©es dans des "directions" d√©coupl√©s les unes des autres ($Cov(a_i,a_j) = 0 \quad\forall i \neq j$)

C'est pour cette raison que les vecteurs propres de nos donn√©es sont int√©ressants.

Intuitivement on se doute bien depuis le d√©but qu'il est int√©ressant d'exprimer nos donn√©es selon un unique axe car elles s'apparent grossi√®rement √† une droite.

---

Pour effectuer la projection il va nous suffire de r√©duire √† z√©ro la dimension de plus petite variance, ce qui reviens √† effectu√© un "changement de base" avec seulement les vecteurs de plus grand variance (dans notre cas, un seul vecteur).

On obtient alors les donn√©es suivantes : 

![projectedData](src\PCA\imgs\projectedData.png)

On peut ramener ensuite nos donn√©es dans notre base d'origine pour comparer avec nos donn√©es d'origine. Cette √©tape est utilis√©e uniquement afin comparer notre projection dans le m√™me "syst√®me de coordonn√©es" que nos donn√©es d'origine ou autrement dit dans la m√™me dimensions dans le sens o√π il s'agit ici d'ajouter une dimension "superflue" √† nos donn√©es ainsi r√©duite.

![projectedData](src\PCA\imgs\backProjectedData.png)

### Le cas r√©el en hautes dimensions

L'ACP est g√©n√©ralement utilis√©e pour √©liminer beaucoup plus de dimensions qu'une seule (comme dans notre exemple 2D). Elle est souvent utilis√©e pour la visualisation des donn√©es, mais aussi pour la r√©duction des caract√©ristiques, c'est-√†-dire pour envoyer moins de donn√©es dans votre algorithme d'apprentissage afin d'am√©liorer ses performances.

Dans notre cas 2D, il √©tait √©videment qu'une seul direction ou dimension √©tait int√©ressante pour exprimer nos donn√©es mais dans le cas de grande dimensions comment savoir combien de dimensions omettre ?En d'autres termes, combien de "composants" dois t'on conserver lorsque de l'ACP ?

Il y a plusieurs fa√ßons de faire ce choix. il faudra g√©n√©ralement faire un compromis entre la pr√©cision et la vitesse de calcul. On peut par exemple exprimer sur un graphique la variance des donn√©es en fonction du nombre de composantes que l'on garde.

Abordons pour finir cette parti sur l'ACP un exemple plus concret illustrant cette m√©thode de s√©lection et un cas "r√©el" d'application de l'ACP.

### Example: MNIST dataset

Prenons un ensemble d'images de 28x28 pixels repr√©sentant des chiffres manuscrits et appliquons l'analyse en composantes principales sur ces images consid√©r√©s comme des vecteurs de 784 composantes (dans un espace de 784 dimensions donc).

![MnistExamples](src\PCA\imgs\MnistExamples.png)

Apr√®s calcul des vecteurs et valeurs propres de nos donn√©es, tra√ßons le graphe des valeurs propres rang√©es par ordres d√©croissantes :

![eigensValuesByComponents](src\PCA\imgs\eigensValuesByComponents.png)

On aper√ßois ici que la valeurs des valeurs propres d√©cro√Æt tr√®s rapidement avec le nombre de composantes. On peut interpr√©ter cela comme une d√©croissance tr√®s rapide de la variances de nos donn√©es expliqu√©s par les composantes ou autrement dit, "peu" de composantes repr√©sentent une parties significative de la variance de nos donn√©es. 

> Rappel: plus la valeurs propre d'un vecteur propre est grande plus la variance expliqu√©e par ce vecteur est grande)

Exprimons maintenant ce m√™me graphique en valeurs cumul√©s et en normalisant les valeurs propres.

![cumulativeExplainedVariance](src\PCA\imgs\cumulativeExplainedVariance.png)

On obtient alors le  graphe de la variance cumulative expliqu√©e en fonction du nombre de composantes.
Ce graphe permet alors de conna√Ætre pour un nombre de composantes donn√©es le pourcentage de variance des donn√©es expliqu√© ce qui peut interpr√©ter comme un pourcentage de pr√©cisions √† repr√©senter les donn√©es d'origines avec ce nombre de composante.
On vois par exemple qu'avec seulement 330 composantes il est possible de repr√©senter √† 99% nos donn√©es d'origine.

---

Pour conclure avec cet exemple, voil√† un tableau de diff√©rents chiffres projet√©s avec diff√©rents niveaux de variance expliqu√© (et le nombre de composantes retenues).
Les chiffres sous les images corresponds √† 3 mesures permettant d'√©valuation de la qualit√© de projection des images √† savoir de haut en bas : le rapport signa-bruit (PSNR), l'erreur quadratique moyenne (MSE) et la structural similarity (SSIM).

![MnistEvaluation](src\PCA\imgs\MnistEvaluation.png)

## Restricted Boltzmann Machines

### L'id√©e

TODO

### √©nergie d'activation

### Apprentissage

TODO



## Solution propos√©e

TODO 

ajout des photo tir√©es du papier, unfolding multicouche, ..



## Comparaison



## Conclusion

r√©sultats obtenus, ..


limitations, erreurs, ouvertures, am√©liorations, papiers plus r√©cents sur la m√™me probl√©matique

## Bibliographie

Vecteur propres : 

[3Blue1Brown]: https://www.youtube.com/watch?v=PFDu9oVAE-g	"Les vecteurs propres et valeurs propres"


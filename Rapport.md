# Reducing the Dimensionality of Data with Neural Networks

by G. E. Hinton and R. R. Salakhutdinov

## Abstract

Des donn√©es en haute dimension peuvent √™tre un probl√®me et n√©cessitent parfois d'√™tre repr√©sent√©es avec moins de dimensions pour des applications multiples comme la visualisation, l'acc√©l√©ration algorithmique ou encore le stockage et la compression de donn√©es. 

Une m√©thode classique et largement utilis√©e dans ce domaine est l'analyse en composantes principales et nous allons expliquer comment l'appliquer.

Nous allons ensuite explorer une m√©thode de r√©duction de dimension bas√©e sur une architecture de neurone multicouche "auto-codeurs". Des algorithmes de descente de gradient sont g√©n√©ralement utilis√©s sur ces architectures mais cela ne fonctionne bien que si les poids initiaux sont proches d'une bonne solution.

Nous allons expliquer et impl√©menter une solution propos√©e par [G. E. Hinton et R. R. Salakhutdinov](www.cs.toronto.edu/~hinton/science.pdf) qui permet d'initialiser efficacement les poids du r√©seau d'auto-encodage profond √† l'aide de plusieurs couches cons√©cutives sur le mod√®le des *Restricted Boltzmann Machines*. Le r√©seaux pourra ensuite affiner ses poids √† l'aide d'un algorithme de descente de gradient afin d'apprendre des codes de faible dimension qui fonctionnent beaucoup mieux qu'un r√©seau d'auto-encodage na√Øf.

Enfin, nous allons comparer les r√©sultats obtenus et conclure sur quelle est la plus efficace des solutions pr√©sent√©es.

<div style="page-break-after: always; break-after: page;"></div>

[TOC]

<div style="page-break-after: always; break-after: page;"></div>

## Introduction

**TODO**

- Presentation of the problem(s). 

- Previous works (at least a few citations). If relevant, include things that you have seen during the lectures.

- Contributions. Why is the studied method different/better/worse/etc. than existing previous works. 

  

<div style="page-break-after: always; break-after: page;"></div>

## Analyse en composantes principales

### L'id√©e

L‚Äôanalyse en composantes principales (ACP) est une technique tr√®s populaire.

En termes simples, l'ACP consiste √† effectuer une transformation de coordonn√©es √† partir des axes arbitraires avec lesquels nos donn√©es sont exprim√©es vers un ensemble d'axes "align√©s avec les donn√©es elles-m√™mes". C'est √† dire, des axes qui expriment au mieux la dispersion des informations pr√©sentes. La dispersion n'est rien d'autre que la variance ou le fait de disposer d'une "information √©lev√©e". On peut dire en d'autres termes qu'une dispersion √©lev√©e contient une information √©lev√©e.

Par cons√©quent, si nos donn√©es sont exprim√©es par des axes maximisant la repr√©sentation de l'information, il est alors possible d'omettre les axes ou les dimensions ayant une variance moindre car ces "composantes" ne participent que tr√®s peu √† la description des donn√©es.
Cela peut par exemple servir √† am√©liorer la rapidit√© de certains algorithmes en √©pargnant de nombreux calculs sans trop souffrir de perte de pr√©cision.

Selon les points de vue, on peut consid√©rer cette analyse comme une technique descriptive o√π l‚Äôon essaie de r√©sumer les donn√©es selon ses dimensions les plus importantes. Cela peut √™tre utilis√© comme une technique de visualisation o√π l‚Äôon essaie de pr√©server la "proximit√©" entre les individus dans un espace de repr√©sentation r√©duit par exemple.

Nous d√©finirons les termes suivants au fur et √† mesure, mais voici le processus r√©sum√© :

- Trouver la matrice de covariance pour l'ensemble de donn√©es en question
- Trouver les vecteurs propres de cette matrice
- Trier les vecteurs propres/"dimensions" de la plus grande √† la plus petite variance
- Projection / R√©duction des donn√©es : Utiliser les vecteurs propres correspondant √† la plus grande variance pour projeter l'ensemble de donn√©es dans un espace √† dimensions r√©duites
- V√©rification : combien avons-nous perdu en pr√©cision dans la repr√©sentation des donn√©es ?

### Variance et covariance

Techniquement, la variance est la moyenne des diff√©rences au carr√© par rapport √† la moyenne. Si vous connaissez bien l'√©cart type, g√©n√©ralement not√© $\sigma$, la variance est juste le carr√© de l'√©cart type. 
$$
V = \sigma^2 = {1\over N}\sum_{i=0}^N (x_i - \bar{x})^2
$$
Avec la moyenne not√©e $\bar{x}$:
$$
\bar{x} = {1\over N}\sum_{i=0}^N x_i
$$
La variance exprime la "propagation" ou "l'√©tendue" des donn√©es.

Exemple d'un jeu de donn√©es 2D :

![plot2D](./src/PCA/imgs/plot2D.png)

Il est possible de calculer la variance selon les deux axes : 

| axe      | x                  | y                   |
| -------- | ------------------ | ------------------- |
| variance | 1.2526627262767291 | 0.31870756461533817 |

On peut remarquer dans le graphique ci-dessus que $x$ varie "avec" $y$ √† peu pr√®s. On dit alors que $y$ est "covariant" avec $x$ . 

La covariance indique le niveau auquel deux variables varient ensemble.
Pour la calculer, c'est un peu comme la variance r√©guli√®re, sauf qu'au lieu d'√©lever au carr√© l'√©cart par rapport √† la moyenne pour une variable, nous multiplions les √©carts pour les deux variables.
$$
Cov(x,y) = {1\over N-1}\sum_{j=1}^N (x_j-\bar x)(y_j-\bar y)
$$

> Remarque : la covariance d'une variable avec elle m√™me est sa variance.

>  <span style="color:red">Remarque :</span> On divise ici par $N-1$ au lieu de $N$, donc contrairement √† la variance r√©guli√®re, nous ne prenons pas tout √† fait la moyenne. Pour un grand ensemble de donn√©es cela ne fait essentiellement aucune diff√©rence, mais pour un petit nombre de points de donn√©es, l'utilisation de ùëÅ peut donner des valeurs qui ont tendance √† √™tre trop petites et donc le $N-1$ a √©t√© introduit pour "r√©duire le biais des petits √©chantillons".

La covariance de $x$ en fonction de $y$ vaut donc dans notre exemple $0.6250769297631616$

### Matrice de covariance

La matrice de covariance est une matrice qui regroupe la variance et covariance de chaque variable ( ou dimension) avec chaque autres et s'exprime sous la forme :
$$
\begin{pmatrix}
   Cov(x, x) & Cov(x, y) \\
   Cov(x, y) & Cov(y, y) \\
\end{pmatrix}
$$
Le long de la diagonale se trouvera la variance de chaque variable (√† un facteur ${(N-1) \over N}$ pr√®s), et le reste de la matrice sera constitu√©e des covariances. 

> Remarque : La matrice est *carr√©e* et puisque l'ordre des variables n'a pas d'importance lors du calcul de la covariance, la matrice sera *sym√©trique*.

On obtient dans notre exemple : 
$$
\begin{pmatrix}
1.25895751 & 0.62507693 \\
0.62507693 & 0.32030911 \\
\end{pmatrix}
$$
Nous avons donc maintenant une matrice de covariance. L'√©tape suivante de l'ACP consiste √† trouver les "composantes principales". Cela signifie les directions dans lesquelles les donn√©es varient le plus. Cela n√©cessite de trouver des vecteurs propres pour la matrice de covariance de notre ensemble de donn√©es.

### Vecteurs propres

D'apr√®s le Th√©or√®me Spectral, pour une matrice ${\bf A}$ *sym√©trique r√©elle* de taille $n\times n$ il existe un ensemble de $n$ vecteurs $\vec{v}_i$ de sorte que la multiplication d'un de ces vecteurs par ${\bf A}$ donne un vecteur proportionnel (d'un facteur $\lambda_i$) √† $\vec{v}_i$.
$$
{\bf A} \vec{v}_i = \lambda_i \vec{v}_i
$$
On appelle ces vecteurs les vecteurs propres et les $\lambda_i$ leurs valeurs propres associ√©es.

Nous ne rentrerons pas dans le d√©tails de comment obtenir ces vecteurs et valeurs propres ici car de nombreuses librairies permettent de le faire et beaucoup mieux que nous.

Il s'agit en g√©n√©ral pour les cas les plus simples de suivre les √©tapes suivantes: 

- Trouver les valeurs propres (en r√©solvant le syst√®me $det( \bf{A} - \lambda \bf{I}) = 0$ )
- Pour chaque valeur propre, obtenir un syst√®me d'√©quations lin√©aires pour chaque vecteur propre et les r√©soudre


On obtient pour notre jeu de donn√©es :

$\lambda_1 = 1.57128949$ et $\lambda_2 = 0.00797714$

$v_1 = \begin{pmatrix}0.89454536 \\ 0.44697717\end{pmatrix}$ et $v_2 = \begin{pmatrix}-0.44697717 \\ 0.89454536\end{pmatrix}$

![plot2DEigensVectors](src\PCA\imgs\plot2DEigensVectors.png)

On remarque bien ici que le vecteur propre $v_1$ indique la direction qui maximise la variance de nos donn√©e. On a donc bien trouv√© ici l'axe "principal" de notre jeu de donn√©es. Le deuxi√®me vecteur propre pointe dans la direction de la plus petite variance et est orthogonal au premier vecteur (cons√©quence du fait que la matrice est sym√©trique r√©elle).

>  Remarque : la longueur des vecteurs est exprim√©e √† l'aide de leurs valeurs propre pour illustrer l'importance des diff√©rents axes dans la variance de nos donn√©es.

### Projection des donn√©es

Nous avons maintenant nos composantes (vecteurs propres), et nous les avons "class√©es" selon leur "importance". Nous allons maintenant √©liminer les directions de faible variance moins importantes. En d'autres termes, nous allons projeter les donn√©es sur les diff√©rentes composantes principales de plus grande variance.

Il est possible par un changement de base d'exprimer nos donn√©es selon ces axes principaux :

![projectedData](src\PCA\imgs\EigenBaseData.png)

La matrice de covariance obtenue avec ces donn√©es exprim√©es dans notre nouvelle base donne : 
$$
\approx
\begin{pmatrix}
	1.57128949 & 0 \\
	0 & 0.00797714 \\
\end{pmatrix}
$$
Ce nouveau "syst√®me de coordonn√©es" exprime les donn√©es dans des "directions" d√©coupl√©es les unes des autres ($Cov(a_i,a_j) = 0 \quad\forall i \neq j$)

C'est pour cette raison que les vecteurs propres de nos donn√©es sont int√©ressants.

Intuitivement on se doutait bien depuis le d√©but qu'il serait int√©ressant d'exprimer nos donn√©es selon un unique axe, puisque elles s'apparentent grossi√®rement √† une droite.

---

Pour effectuer la projection il va nous suffire de r√©duire √† z√©ro la dimension de plus petite variance, ce qui reviens √† effectu√© un "changement de base" avec seulement les vecteurs de plus grande variance (dans notre cas, un seul vecteur).

On obtient alors les donn√©es suivantes : 

![projectedData](src\PCA\imgs\projectedData.png)

On peut ramener ensuite nos donn√©es dans notre base d'origine pour comparer avec nos donn√©es d'origine. Cette √©tape est utilis√©e uniquement afin de comparer notre projection dans le m√™me "syst√®me de coordonn√©es" que nos donn√©es d'origine ou autrement dit dans la m√™me dimension dans le sens o√π il s'agit ici d'ajouter une dimension "superflue" √† nos donn√©es ainsi r√©duite.

![projectedData](src\PCA\imgs\backProjectedData.png)

### Le cas r√©el en hautes dimensions

L'ACP est g√©n√©ralement utilis√©e pour √©liminer beaucoup plus de dimensions qu'une seule (comme dans notre exemple 2D). Elle est souvent utilis√©e pour la visualisation des donn√©es, mais aussi pour la r√©duction des caract√©ristiques, c'est-√†-dire pour envoyer moins de donn√©es dans votre algorithme d'apprentissage afin d'am√©liorer ses performances.

Dans notre cas 2D il √©tait √©vident qu'une seule direction ou dimension √©tait int√©ressante pour exprimer nos donn√©es ; mais dans les cas de grande dimension, comment savoir combien de dimensions omettre ? En d'autres termes, combien de "composants" doit-on conserver lors de l'ACP ?

Il y a plusieurs fa√ßons de faire ce choix. Il faudra g√©n√©ralement faire un compromis entre la pr√©cision et la vitesse de calcul. On peut par exemple exprimer sur un graphique la variance des donn√©es en fonction du nombre de composantes que l'on garde.

Abordons pour finir cette partie sur l'ACP un exemple plus concret illustrant cette m√©thode de s√©lection et un cas "r√©el" d'application de l'ACP.

### Example: MNIST dataset

Prenons un ensemble d'images de 28x28 pixels repr√©sentant des chiffres manuscrits et appliquons l'analyse en composantes principales sur ces images consid√©r√©es comme des vecteurs de 784 composantes (dans un espace de 784 dimensions donc).

![MnistExamples](src\PCA\imgs\MnistExamples.png)

Apr√®s calcul des vecteurs et valeurs propres de nos donn√©es, tra√ßons le graphe des valeurs propres rang√©es par ordre d√©croissant :

![eigensValuesByComponents](src\PCA\imgs\eigensValuesByComponents.png)

On aper√ßoit ici que les valeurs des valeurs propres d√©croissent tr√®s rapidement. On peut interpr√©ter cela comme une d√©croissance tr√®s rapide de la variances de nos donn√©es expliqu√©es par les composantes ; ou autrement dit, "peu" de composantes repr√©sentent une partie significative de la variance de nos donn√©es. 

> Rappel : plus la valeur propre d'un vecteur propre est grande plus la variance expliqu√©e par ce vecteur est grande

Exprimons maintenant ce m√™me graphique en valeurs cumul√©es et en normalisant les valeurs propres.

![cumulativeExplainedVariance](src\PCA\imgs\cumulativeExplainedVariance.png)

On obtient alors le  graphe de la variance cumulative expliqu√©e en fonction du nombre de composantes.
Ce graphe permet alors de conna√Ætre pour un nombre de composantes donn√© le pourcentage de variance des donn√©es expliqu√©, ce qui peut s'interpr√©ter comme un pourcentage de pr√©cision √† repr√©senter les donn√©es d'origine avec ce nombre de composantes.
On voit par exemple qu'avec seulement 330 composantes il est possible de repr√©senter √† 99% nos donn√©es d'origine.

---

Pour conclure avec cet exemple, voici un tableau de diff√©rents chiffres projet√©s avec diff√©rents niveaux de variance expliqu√©e (et le nombre de composantes retenues).
Les chiffres sous les images correspondent √† trois mesures permettant l'√©valuation de la qualit√© de projection des images ; √† savoir de haut en bas : le rapport signa-bruit (PSNR), l'erreur quadratique moyenne (MSE) et la structural similarity (SSIM).

![MnistEvaluation](src\PCA\imgs\MnistEvaluation.png)

<div style="page-break-after: always; break-after: page;"></div>

## Restricted Boltzmann Machines

Invent√©es par Geoffrey Hinton, les machines de Boltzmann restreintes sont des r√©seaux neuronaux peu profonds √† deux couches qui constituent les √©l√©ments de base des r√©seaux profonds.

### L'id√©e

Une RBM est utilis√©e pour avoir une estimation de la distribution probabiliste d'un jeu de donn√©es.

La premi√®re couche de la RBM est appel√©e la couche visible, ou couche d'entr√©e, et la seconde est la couche cach√©e.

Nous d√©taillerons ici plus pr√©cis√©ment le fonctionnement d'une RBM de Bernoulli qui consid√®re les unit√©s visibles et cach√©es comme √©tant des valeurs binaires.

### √ânergie d'activation

On d√©finit l'√©nergie d'activation d'une machine de Boltzmann restreinte par la formule suivante :
$$
E(\bold v, \bold h) = -\sum_i b_iv_i - \sum_j c_jh_j - \sum_{i,j} w_{ij}v_ih_j
$$
ou matriciellement avec :
$$
E(\bold v, \bold h) = -b^T{\bold v} - c^T{\bold h} - {\bold v}W{\bold h}^T
$$
avec :

- $w_{ij}$ le poids entre le neurone $j$ et le neurone $i$ ($W$)
- $v_i$ l'√©tat du neurone $i$ de la couche visible ($\bold v$)
- $h_j$ l'√©tat du neurone $j$ de la couche cach√©e ($\bold h$)
- $b_i$ et $c_j$ sont les biais des neurones $v_i$ et $h_j$ d'entr√©e et de sortie

Cette √©nergie est interpr√©t√©e comme l'√©nergie d'un syst√®me physique et on peut d√©finir le score d'une configuration √©nerg√©tique comme l'inverse de cette √©nergie. Plus l'√©nergie est faible (stabilit√©) plus le score est √©lev√©.
$$
\text{Score} = - E
$$

### Probabilit√©

Consid√©rons des scores donn√©s pour des configurations possibles de notre syst√®me.
$$
[ 0, 1, 2, 5, 8]
$$
Il est int√©ressant d'exprimer le score de ces configurations en probabilit√©s et le moyen le plus classique est de normaliser ces scores par la somme de tous les scores :

![scoresProba](src\RBM\imgs\scoresProba.png)

Cependant des probl√®mes peuvent survenir avec des scores n√©gatifs car ils peuvent se compenser et la somme peut alors √™tre nulle.

Un moyen naturel peut √™tre de passer √† l'exponentielle puis de normaliser, c'est ce qu'on appelle l'op√©ration de softmax.

![softmax](src\RBM\imgs\softmax.png)

C'est ainsi que l'on peut d√©finir la probabilit√© d'avoir une certaine configuration entr√©e-sortie $(\bold v, \bold h)$ :
$$
P(\bold v, \bold h) = {e^{-E(\bold v, \bold h)} \over Z}
$$
o√π $Z$ est une constante de normalisation d√©finie ainsi:
$$
Z=\sum_{i,j} e^{-E(\bold v, \bold h)}
$$


### Positionnement du probl√®me

L'id√©e g√©n√©rale est de modifier les poids $w_{ij}$ pour approcher au mieux la distribution de probabilit√© de nos donn√©es.
C'est diff√©rent d'un algorithme plus classique comme une r√©gression par exemple, qui estime une valeur continue bas√©e sur de nombreuses entr√©es.

En ajustant it√©rativement les poids en fonction de l'erreur qu'ils produisent ou de leurs scores, une RBM apprend √† se rapprocher de la distribution de probabilit√© des donn√©es originales. On pourrait dire que les poids en viennent √† refl√©ter la structure de l'entr√©e au travers des probabilit√©s de la couche cach√©e.

Consid√©rons un exemple simple dans lequel une personne dispose de trois accessoires :

Une paire de lunette (not√©e **L**), un parapluie (not√© **P**) et une cam√©ra (not√©e **C**)

Regardons ce qu'elle d√©cide ou non de prendre lorsqu'elle sort de chez elle en fonction des jours de la semaine :

| jour | Lunettes :eyeglasses: | Parapluie :closed_umbrella: | Cam√©ra :camera:    |
| ---- | --------------------- | --------------------------- | ------------------ |
| 0    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 1    | :x:                   | :heavy_check_mark:          | :x:                |
| 2    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 3    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 4    | :x:                   | :heavy_check_mark:          | :x:                |
| 5    | :x:                   | :heavy_check_mark:          | :x:                |
| 6    | :x:                   | :heavy_check_mark:          | :heavy_check_mark: |
| 7    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 8    | :heavy_check_mark:    | :x:                         | :heavy_check_mark: |
| 9    | :x:                   | :heavy_check_mark:          | :x:                |

Ces donn√©es vont constituer nos donn√©es d'entr√©e de la couche d'input.

Consid√©rons maintenant qu'il existe deux facteurs qui pourraient expliquer ces donn√©es : la pr√©sence ou non de soleil :high_brightness: (not√© **S**) et d'averse :sweat_drops: ‚Äã(not√© **A**) au cours de la journ√©e. ‚Äã

Ces facteurs vont constituer la couche cach√©e de notre syst√®me.

Initialisons maintenant tous nos poids √† 0 et tra√ßons la probabilit√© de chaque configuration d'entr√©e-sortie.

> Nous ignorerons les biais associ√©s aux entr√©es et sorties dans notre exemple pour plus de simplicit√©.

> Une configuration est not√©e avec une suite de lettres montrant la pr√©sence ou non de l'accessoire et d'un √©v√®nement m√©t√©orologique.

![uniformProbabilities](src\RBM\imgs\uniformProbabilities.png)

On constate √©videment que toutes les configurations sont √©quiprobables (car tous nos poids sont nuls et donc l'√©nergie de chaque configuration est nulle)

On aimerait se rapprocher d'une configuration de probabilit√©s qui repr√©sente nos donn√©es, c'est-√†-dire qui exprime les formations possibles de notre jeu de donn√©es.

Dans notre cas voici les configurations qui apparaissent tout au long de la semaine concernant les inputs : 

| jour          | 0                     | 1                 | 2                     | 3                     | 4                 | 5                 | 6                         | 7                     | 8                     | 9                 |
| ------------- | --------------------- | ----------------- | --------------------- | --------------------- | ----------------- | ----------------- | ------------------------- | --------------------- | --------------------- | ----------------- |
| configuration | :eyeglasses: :camera: | :closed_umbrella: | :eyeglasses: :camera: | :eyeglasses: :camera: | :closed_umbrella: | :closed_umbrella: | :closed_umbrella::camera: | :eyeglasses: :camera: | :eyeglasses: :camera: | :closed_umbrella: |

En incluant les configurations possibles de notre couche cach√©e, il est possible de lister ainsi toutes les configurations (entr√©e et sortie) envisageables:

- :eyeglasses: :camera:
- :eyeglasses: :camera::sweat_drops:
- :eyeglasses: :camera::high_brightness:
-  :eyeglasses: :camera::high_brightness::sweat_drops: 
- :closed_umbrella:
- :closed_umbrella::sweat_drops:
- :closed_umbrella::high_brightness:
-  :closed_umbrella::high_brightness::sweat_drops: 
- :closed_umbrella: :camera:
- :closed_umbrella: :camera::sweat_drops:
- :closed_umbrella: :camera::high_brightness:
- :closed_umbrella: :camera::high_brightness::sweat_drops: 

On aimerait donc trouver les poids de notre algorithme pour que les configurations ou √©v√®nements envisageables aient une grande probabilit√© et les autres une faible probabilit√©.

Cela donnerait quelque chose comme cela pour notre exemple :

![exempleWantedProbabilities](src\RBM\imgs\exempleWantedProbabilities.png)

> L'√©v√®nement :eyeglasses: :camera: (LC) a lieu 5 fois dans la semaine. Il est donc normal que cette configuration soit plus probable que l'√©v√®nement :closed_umbrella: (P) (qui a lieu 3 fois) ou que l'√©v√®nement :closed_umbrella: :camera: (PC) (qui a lieu une unique fois).


### Apprentissage

####  *Contrastive Divergence*

Il faut donc faire converger nos probabilit√©s uniformes initiales (poids nuls) vers les probabilit√©s voulues issues des √©v√®nements de notre dataset.
Pour cela, on va donc proc√©der par it√©rations sur chaque √©v√®nement de notre dataset afin d'augmenter la probabilit√© de l'√©v√®nement consid√©r√© et diminuer la probabilit√© de tous les autres √©v√®nements.
Cela donne par exemple pour le 1er jour (1er √©v√®nement de notre dataset) :

![jour1](src\RBM\imgs\jour1.png)

Voil√† ce que cela donne si on effectue ce processus it√©ratif une fois sur chacun des jours de notre dataset:

![contrastiveDivergence](src\RBM\imgs\contrastiveDivergence.png)

<div style="page-break-after: always; break-after: page;"></div>

### Gibbs sampling

Un probl√®me de taille s'impose malheureusement √† nous.
En pratique il est impossible de consid√©rer ni m√™me de stocker les √©v√®nements dont nous parlons depuis le d√©but.
En effet, il y a $2^{M + N}$ combinaisons d'√©v√®nements possible pour une **RBM** ayant une couche visible de taille $M$ et une une couche cach√©e de taille $N$.

Dans notre exemple cela reste relativement raisonnable : $2^{2+3} = 32$ mais en pratique avec une couche visible de taille **200** et une couche cach√©e de taille **100** cela nous am√®ne √† consid√©rer $2^{300}$ √©v√®nements ce qui est plus que le nombre d'atomes dans l'univers observable.

Au lieu d'augmenter la probabilit√© de tous les √©v√®nements probables en fonction de notre donn√©e d'entr√©e on va plut√¥t en s√©lectionner un de mani√®re al√©atoire et augmenter sa probabilit√©. Puis choisir un autre √©v√®nement de mani√®re "al√©atoire" et diminuer sa probabilit√©. Ce processus de deux √©tapes, si r√©p√©t√© suffisamment, va permettre d'approcher le r√©sultat voulu.

Cependant, nous ne voulons pas exactement s√©lectionner un √©v√®nement al√©atoirement dans ces deux √©tapes. 
En effet, il est plus judicieux d'augmenter la probabilit√© d'une √©v√®nement possible suivant une entr√©e donn√©e de notre dataset si sa probabilit√© actuelle est faible. De m√™me, il est souhaitable de diminuer la probabilit√© d'un √©v√®nement actuellement plus probable dans la seconde √©tape.

---

On peut s'apercevoir que les probabilit√©s conditionnelles des √©v√®nements cach√©s et visibles sont ind√©pendants lorsque les valeurs de l‚Äôautre couche sont fix√©es:
$$
p(h_j = 1 | \bold v) = \sigma(b_i +\sum_i v_iw_{ij})
$$

$$
p(v_i = 1 | \bold h) = \sigma(c_j +\sum_j h_jw_{ij})
$$

avec :
$$
\sigma(x) = { 1 \over 1+ e^{-x}}
$$
Comme on l'a √©voqu√© pr√©c√©demment, il est impossible de consid√©rer ni m√™me de calculer la probabilit√© de chaque √©v√©nement pour choisir de diminuer celui qui poss√®de la plus grand probabilit√©.

On va donc devoir effectuer un √©chantillonnage de Gibbs it√©rativement en ayant connaissance des probabilit√©s conditionnelles ind√©pendantes pour une couche en faisant des aller-retours entre les deux couches comme cela :
$$
h_j^{(n+1)} \sim p(h_j | \bold v^{(n)})  \\
v_i^{(n+1)} \sim p(v_i | \bold h^{(n+1)})
$$
En principe $n$ doit √™tre grand pour que l'√©chantillonnage soit efficace et refl√®te r√©ellement un tirage al√©atoire. En pratique, ces erreurs sont n√©gligeables et vont √™tre compens√©es par la suite lors de l'utilisation de ces RBMs dans un processus de fine-tuning par descente de gradient.

---

### R√©capitulatif

On peut reformuler l'apprentissage math√©matique dans le sens o√π l'on cherche √† trouver les poids (et biais) maximisant le produit des probabilit√©s des √©v√®nements de notre jeu de donn√©es ($D$).

$$
arg \; \underset{W}{max}\;\underset{\bold v \in D}{\Pi}p(\bold v)
$$

o√π $p(\bold v) = \sum_{\bold h} p(v,h)$ est la distribution marginale des variables visibles $\bold v$ de la RBM.

On peut enfin exprimer la d√©riv√©e n√©cessaire pour faire √©voluer les poids de notre RBM:
$$
{\partial \over \partial W}log(P(v_n)) = \mathbb{E}\left[ {\partial \over \partial W} - E(v, h) | v= v_n \right] - \mathbb{E}\left[ {\partial \over \partial W} - E(v, h) \right]
$$

> **Remarque :**
>
> Jusqu'√† pr√©sent nous avons consid√©r√© des valeurs d'entr√©e et de sortie binaires. Mais cela pose un probl√®me lorsque l'on s'int√©resse √† des donn√©es plus complexes (continues) comme des images par exemple.
>
> Apr√®s quelques modifications il est possible de g√©n√©raliser et consid√©rer des valeurs continues sous quelques contraintes de normalisation de ces valeurs (sous forme de distribution gaussienne par exemple).

<div style="page-break-after: always; break-after: page;"></div>

## Solution propos√©e

Le but de de la recherche propos√©e est de r√©duire la dimensionnalit√© de nos donn√©es sur la base d'une structure de r√©seaux auto-encodeurs.

Les auto-encodeurs sont un type sp√©cifique de r√©seaux neuronaux o√π la taille de l'entr√©e est la m√™me que la sortie. Ils compriment l'entr√©e en un code de dimension inf√©rieure et reconstruisent ensuite la sortie √† partir de cette repr√©sentation. Le code est un "r√©sum√©" compact ou une "compression" de l'entr√©e, √©galement appel√©e repr√©sentation en espace latent (latent-space).
Un auto-codeur se compose donc de trois √©l√©ments : l'encodeur, le code et le d√©codeur. L'encodeur compresse l'entr√©e et produit le code, le d√©codeur reconstruit ensuite l'entr√©e uniquement √† l'aide de ce code.

![autoEncoder](imgs\autoEncoder.png)

Cependant, il est difficile d'optimiser les poids dans les auto-encodeurs non-lin√©aires qui ont de nombreuses couches cach√©es. Avec des poids initiaux √©lev√©s, ils trouvent g√©n√©ralement de mauvais minima locaux ; avec des poids initiaux faibles, les gradients dans les premi√®res couches sont tr√®s faibles, ce qui rend impossible l'entra√Ænement efficace d'auto-encodeurs avec de nombreuses couches cach√©es. 

Si les poids initiaux sont proches d'une bonne solution, l'apprentissage et la descente de gradient fonctionnent bien. L'id√©e est d'utiliser plusieurs RBM en amont pour se rapprocher d'une bonne solution et d'affiner les poids du r√©seau par descente classique ensuite.

L'architecture propos√©e est constitu√©e de diff√©rentes RBMs successives de plus en plus petites afin de r√©duire en plusieurs √©tapes la dimensionnalit√© des donn√©es. 
Ces RMBs sont ensuite utilis√©es dans l'autre sens (unfolding multicouche) pour constituer la partie que l'on appelle d√©codeur et qui permet de reconstituer les donn√©es √† partir de la repr√©sentation en dimension inf√©rieure que l'on appelle le code (ou la compression).

![image-20210205113440207](imgs\RMBAutoEncoder.png)



<div style="page-break-after: always; break-after: page;"></div>

## Comparaison et r√©sultats

Pour tester nos r√©sultats nous avons fait le choix d'utiliser le jeu de donn√©es bien connu **MNIST** qui nous donne acc√®s √† des images de 28x28 pixels repr√©sentant des chiffres √©crits √† la main. 

### Mod√®le 784-1000-500-250-3

Dans un premier temps nous avons test√© une architecture propos√©e dans le papier de recherche en changeant seulement la taille de la derni√®re couche d'encodage (3 dimensions au lieu de 2) pour pouvoir visualiser le r√©sultat en 3D. Cela va nous permettre de comparer avec une analyse en composante principale (PCA) en conservant les trois premi√®res composantes (ce qui reste tr√®s peu).

**PCA**

![PCA3Projection](src\RBM\imgs\PCA3Projection.png)

On remarque quelques s√©parations comme les "1" regroup√©s en bleu marine en bas √† droite et les "0" en marron en haut. Cependant, les trois premi√®res composantes repr√©sentent seulement 29,1% de la variance des donn√©es ce qui est trop peu pour les repr√©senter correctement en si peu de de dimensions. Voil√† ce que la reconstruction donne en images :

![RBMReconstruct(784-1000-500-250-3)](src\RBM\imgs\PCA(3).png)

On se rend d'autant plus compte ici que la reconstruction n'est pas du tout fid√®le aux donn√©es d'origine et n'est pas pertinente pour de la compression.

**RBM**

![RBM3Projection](src\RBM\imgs\RBM3Projection.png)

Ici, le r√©sultat est vraiment plus satisfaisant. Les domaines des diff√©rents chiffres sont bien distincts en 3D.

Nous obtenons une erreur quadratique moyenne (MSE) de 19.95 pour les donn√©es d'entra√Ænement contre 21.7 sur les donn√©es de test. Cela peut s'expliquer facilement par le fait qu'il n'y a pas assez de dimensions pour encoder toute la complexit√© de nos donn√©es malgr√© une repr√©sentation satisfaisant en seulement 3 dimensions. Voil√† ci dessous les reconstructions obtenues. 

> On remarque qu'elles sont tout de m√™me assez "floues" par manque de complexit√© dans la repr√©sentation compress√©e.

![RBMReconstruct(784-1000-500-250-3)](src\RBM\imgs\RBMReconstruct(784-1000-500-250-3).png)

### Mod√®le 784-1000-500-250-100-30

Il est int√©ressant maintenant de tester cette approche dans une r√©duction de dimension raisonnable en pratique et qui devrait donner de meilleurs r√©sultats. Le mod√®le que nous allons tester est une l√©g√®re modification d'un mod√®le propos√© dans le papier et est constitu√© de couches dont les tailles successives sont : 

$$
 784-1000-500-250- 100-30
$$
Une dimension de 30 √©l√©ments pour le code repr√©sentant nos donn√©es devrait √™tre suffisant pour conserver la complexit√© des donn√©es du dataset MNIST.

**PCA**

![PCA(30)](src\RBM\imgs\PCA(30).png)

La reconstruction obtenue gr√¢ce √† l'analyse en composantes principales est l√©g√®rement meilleure, cependant les 30 dimensions repr√©sentent seulement 74,6 % de la variance de nos donn√©es. Cela reste trop peu et on l'observe clairement dans la reconstruction ci-dessus.

**RBM**

Regardons ce que cela donne en utilisant les RBMs et la structure propos√©e :

![RBMReconstruct(784-1000-500-250-100-30)](src\RBM\imgs\RBMReconstruct(784-1000-500-250-100-30).png)

La reconstruction sur la gauche est nettement plus convaincante avec une erreur moyenne (MSE) de 2.2 pour les donn√©es d'entra√Ænement et de 3.5 pour les donn√©es de test ce qui est vraiment satisfaisant. Ici la dimension d'encodage permet de conserver la complexit√© des donn√©es tout en r√©duisant drastiquement la dimensionnalit√© de celles-ci de pr√®s de ${784\over30} \approx 26$ fois !

## Conclusion

Comme nous venons de le voir, l'approche de pr√©-apprentissage en utilisant des RBMs s'av√®re tr√®s convaincante. Cela permet d'entrainer un r√©seau auto-encodeur dans un temps raisonnable tout en produisant des r√©sultats honorables.

Pour √©galer ces r√©sultats, une approche par composantes principales devrait quant √† elle conserver pr√®s de 450 (sur 784) dimensions pour le dataset MNIST contre seulement 30 pour l'approche propos√©e.

Cette r√©duction est int√©ressante et a plusieurs applications dont deux importantes en pratique:

- Le stockage et la compression pure des donn√©es
- L'am√©lioration de la rapidit√© d'entra√Ænement d'un algorithme de classification

Une am√©lioration pertinente serait d'utiliser une autre fonction d'erreur plus adapt√©e aux images plut√¥t que la MSE utilis√©e jusqu'√† pr√©sent. En effet des erreurs comme le PSNR ou la SSIM pourraient √™tre qualitativement plus pertinentes pour comparer des images.

Une force de cette approche par rapport √† une approche na√Øve d'un auto-encodeur par descente de gradient est le fait que l'√©tape de pr√©-entra√Ænement par RBM permet au mod√®le d'√™tre construit directement autour des donn√©es √† mod√©liser.

> "Pretraining helps generalization because it ensures that most of the information in the weights comes from modeling the images".

Cette approche permet de contre balancer l'un des point faible des auto-encodeur √† savoir la complexit√© en temps de l'entra√Ænement par rapport √† la taille des donn√©s 

> "All three conditions are now satisfied. Unlike nonparametric methods (15, 16), autoencoders give mappings in both directions between the data and code spaces, and they can be applied to very large data sets because both the pretraining and the fine-tuning scale linearly in time and space with the number of training cases."

Bien que les RBMs soient parfois encore utilis√©es, cette technique devient peu √† peu d√©pr√©ci√©e au profit de r√©seaux adversaires g√©n√©rateurs (GAN) ou d'auto-codeurs variationnels (VAE) plus performants et permettant d'obtenir des r√©sultats plus int√©ressants notamment du point de vue de la g√©n√©ration de donn√©es et de la repr√©sentation de l'espace latent.

On peut citer quelques papiers de recherche qui abordent ces sujets : 

- https://www.worldscientific.com/doi/10.1142/S1469026820500029
- https://arxiv.org/abs/2002.10464
- https://www.biorxiv.org/content/10.1101/2020.07.17.207993v1.full.pdf

<div style="page-break-after: always; break-after: page;"></div>

## Bibliographie

[G. E. Hinton* and R. R. Salakhutdinov]: https://www.cs.toronto.edu/~hinton/science.pdf	"Reducing the Dimensionality of Data with Neural Networks"
[Luis Serrano]: https://www.youtube.com/watch?v=Fkw0_aAtwIw	"Restricted Boltzmann Machines - A friendly introduction"
[3Blue1Brown]: https://www.youtube.com/watch?v=PFDu9oVAE-g	"Les vecteurs propres et valeurs propres"

[Geoffrey Hinton]: http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf	"A Practical Guide to Training Restricted Boltzmann Machines"
[Geoffrey E. Hinton and Simon Osindero]: http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf	"A fast learning algorithm for deep belief nets"
[Chris Nicholson]: https://wiki.pathmind.com/restricted-boltzmann-machine	"A Beginner's Guide to Restricted Boltzmann Machines"

<div style="page-break-after: always; break-after: page;"></div>

## Annexes

Visualisation des √©tapes d'entra√Ænement des RBMs pour le mod√®le **784-1000-500-250-100-30** :

> √† gauche les donn√©es et √† droite leur reconstruction par RBM

![RBMTraining_100-30](src\RBM\imgs\RBMTraining_784-1000.png)

![RBMTraining_100-30](src\RBM\imgs\RBMTraining_1000-500.png)

![RBMTraining_100-30](src\RBM\imgs\RBMTraining_500-250.png)

![RBMTraining_100-30](src\RBM\imgs\RBMTraining_250-100.png)

![RBMTraining_100-30](src\RBM\imgs\RBMTraining_100-30.png)

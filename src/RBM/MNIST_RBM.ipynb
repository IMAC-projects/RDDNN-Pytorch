{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Custom Models\n",
    "from RBM import RBM\n",
    "from DAE import DAE\n",
    "\n",
    "# Display 3D plot\n",
    "import plotly.graph_objects as go\n",
    "\n",
    " # PCA\n",
    "from scipy.linalg import eigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom \n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [12, 12]\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "mpl.rcParams['savefig.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllBatchData(dataLoader):\n",
    "    data = []\n",
    "    targets = []\n",
    "    for d, t in dataLoader:\n",
    "        data.append(d.numpy())\n",
    "        targets.append(t.numpy())\n",
    "    data = np.concatenate(data)\n",
    "    targets = np.concatenate(targets)\n",
    "    return data, targets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenTransform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "MNISTTrain = torchvision.datasets.MNIST(root=\"./dataset/\", train=True, transform=flattenTransform, download=True)\n",
    "MNISTTest = torchvision.datasets.MNIST(root=\"./dataset/\", train=False, transform=flattenTransform, download=True)\n",
    "\n",
    "trainDataLoader = DataLoader(MNISTTrain, batch_size=128, shuffle=True)\n",
    "testDataLoader = DataLoader(MNISTTest, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayOutput(a, b, dim=(28, 28), title=None, fileName=None):\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    \n",
    "    for i, data in enumerate((a, b)):\n",
    "        viewAsImage = data.view(data.shape[0], 1, dim[0], dim[1])\n",
    "        img = torchvision.utils.make_grid(viewAsImage.data).detach().cpu().numpy()\n",
    "        axs[i].imshow(np.transpose(img, (1, 2, 0)))\n",
    "        axs[i].set_axis_off()\n",
    "        axs[i].autoscale(enable=True)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    if title is not None: fig.suptitle(title)\n",
    "    if fileName is not None: plt.savefig(fileName)\n",
    "    plt.show()\n",
    "\n",
    "def makeGrid(array, dim=(28, 28), nCols=8):\n",
    "    width, height = dim\n",
    "    nindex, dim = array.shape\n",
    "    assert width*height == dim\n",
    "    nRows = nindex//nCols\n",
    "    assert nindex == nRows*nCols\n",
    "    return (array.reshape(nRows, nCols, height, width)\n",
    "              .swapaxes(1,2)\n",
    "              .reshape(height*nRows, width*nCols))\n",
    "    return result\n",
    "\n",
    "def displayOutputNumpy(a, b, dim=(28, 28), title=None, fileName=None, cmap=cm.gray):\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    \n",
    "    for i, data in enumerate((a, b)):\n",
    "        axs[i].imshow(makeGrid(data, dim, nCols = 8), cmap=cmap)\n",
    "        axs[i].set_axis_off()\n",
    "        axs[i].autoscale(enable=True)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    if title is not None: fig.suptitle(title)\n",
    "    if fileName is not None: plt.savefig(fileName)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom batch MSE loss\n",
    "class batchMSELoss(torch.nn.Module):\n",
    "    def __init__(self, batchSize):\n",
    "        super(batchMSELoss, self).__init__()\n",
    "        self.batchSize = batchSize\n",
    "        self.mse = torch.nn.MSELoss(reduction='sum')\n",
    "    def forward(self, outputs, targets):\n",
    "        return self.mse(outputs, targets) / self.batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRBM(rbm, loss, dataLoader, epochs, learningRate, weightDecay=2e-4):\n",
    "    paddingLength = 1+int(math.log10(epochs)) # for padded print\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epochLoss = 0\n",
    "        for i, (data, _) in enumerate(dataLoader): # we will use only data\n",
    "            Vp0 = data.to(device)\n",
    "            # Vs0 = torch.bernoulli(Vp0)\n",
    "            # V: Visible | H: Hidden\n",
    "            # s: sampling | p: probabilities\n",
    "            # 0: start | k:end\n",
    "            \n",
    "            Vpk, Vsk = rbm.gibbsSampling(Vp0, iterations = 1) #Vs0\n",
    "              \n",
    "            Hp0, _ = rbm.sampleHidden(Vp0) #Vs0\n",
    "            Hpk, _ = rbm.sampleHidden(Vpk) #Vsk\n",
    "            \n",
    "            rbm.contrastiveDivergence(Vp0, Vpk, Hp0, Hpk, learningRate = learningRate, weightDecay = weightDecay, momentumDamping = 0.5 if epoch < 5 else 0.9)\n",
    "\n",
    "            epochLoss += loss(Vp0, Vpk)\n",
    "\n",
    "        if epoch % (epochs/10) == (epochs/10-1):\n",
    "            print(f\"  Epoch[{epoch+1:>{paddingLength}}] Avg. Loss: {epochLoss / len(dataLoader):.5f}\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genNewDataSet(rbm, device, dataLoader):\n",
    "    # rederive new data loader based on hidden activations of trained model\n",
    "    newData = []\n",
    "    for data, _ in dataLoader:\n",
    "        Hp, _ = rbm.sampleHidden(data.to(device))\n",
    "        newData.append(Hp.detach().cpu().numpy())\n",
    "    newData = np.concatenate(newData)\n",
    "    fakesLabels = np.zeros((len(newData), 1))\n",
    "    return DataLoader(TensorDataset(torch.Tensor(newData).to(device), torch.Tensor(fakesLabels).to(device)), batch_size=dataLoader.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSquareDim(size): #used to find the rectangle dim close as possible to a square that length*width = size\n",
    "    start = int(math.sqrt(size))+1\n",
    "    end = start-1\n",
    "    while start * end != size:\n",
    "        start-=1\n",
    "        while start * end < size:\n",
    "            end+=1\n",
    "            # print(f\"({start}, {end}) {start * end}\")\n",
    "    return (start, end)\n",
    "        \n",
    "# use a Gaussian distribution for the last hidden layer to let it take advantage of continuous values\n",
    "def layersInfos(layersDim, epochsF = lambda x: (x+1)*10, lrF = lambda x: 10**(-x/2-1)):\n",
    "    layersInfos = []\n",
    "    for i, (vDim, hDim) in enumerate(zip(layersDim[:-1], layersDim[1:])):\n",
    "        displayWidth = int(math.sqrt(vDim))\n",
    "        layersInfos.append({\n",
    "            'visibleDim': vDim,\n",
    "            'hiddenDim': hDim,\n",
    "            'epochs': epochsF(i),\n",
    "            'learningRate': lrF(i),\n",
    "            'useGaussian': i == len(layersInfos)-1,\n",
    "            'displayDim': bestSquareDim(vDim)\n",
    "        })\n",
    "    return layersInfos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RBM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainsRBMLayers(layersDim, loss, dataLoader, Display = True, Verbose = True, savePrefixe = None):\n",
    "    RBMLayers = []\n",
    "    \n",
    "    if Verbose:\n",
    "        print(f\"Train layers : {layersDim}\")\n",
    "    for config in layersInfos(layersDim, epochsF = lambda x: (x+1)*5):\n",
    "        if Verbose:\n",
    "            print(f\"\\nTrain layer {config['visibleDim']}-{config['hiddenDim']} ({config['epochs']} Epochs):\")\n",
    "        # create rbm layers\n",
    "        rbm = RBM(device, config['visibleDim'], config['hiddenDim'], config['useGaussian'], useMomentum = True)\n",
    "        \n",
    "        if Verbose: # print initial loss\n",
    "            data = next(iter(dataLoader))[0].to(device)\n",
    "            reconstructedVp, _ = rbm.reconstruct(data)\n",
    "            print(f\"  Initial batch loss: {loss(data, reconstructedVp):.5f}\")\n",
    "        \n",
    "        trainRBM(rbm, loss, dataLoader, config['epochs'], config['learningRate'], weightDecay=2e-4)\n",
    "        \n",
    "        # display sample output\n",
    "        if Display:\n",
    "            \n",
    "            data = next(iter(dataLoader))[0].to(device)\n",
    "            reconstructedVp, _ = rbm.reconstruct(data)\n",
    "            displayOutput(data, reconstructedVp, config['displayDim'], \n",
    "                        title=f\"train layer {config['visibleDim']}-{config['hiddenDim']} Avg MSE: {loss(data, reconstructedVp).item()}\",\n",
    "                        fileName = f\"./imgs/training/{savePrefixe}_RBMTraining_{config['visibleDim']}-{config['hiddenDim']}.png\" if savePrefixe is not None else None)\n",
    "        RBMLayers.append(rbm)\n",
    "        dataLoader = genNewDataSet(rbm, device, dataLoader) # generate new data based on this layer\n",
    "    \n",
    "    return RBMLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBMLayers = TrainsRBMLayers([784, 1000, 500, 250, 3], batchMSELoss(trainDataLoader.batch_size), trainDataLoader, savePrefixe = \"01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & fine-tune autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainPytorchModel(model, optimizer, loss, dataLoader, epochs, Verbose = True):\n",
    "    epochsLoss = []\n",
    "    paddingLength = 1+int(math.log10(epochs)) # for padded print\n",
    "    for epoch in range(epochs):\n",
    "        epochLoss = 0\n",
    "        for batchIdx, (data, _) in enumerate(dataLoader):\n",
    "            data = data.to(device) # to device\n",
    "\n",
    "            optimizer.zero_grad() # zero the parameters gradients\n",
    "\n",
    "            outputs = model(data) # forward\n",
    "            # outputs = torch.tensor(self.prediction.iloc[idx, :],dtype=torch.long)\n",
    "\n",
    "            lossValue = loss(data, outputs) # compute loss\n",
    "\n",
    "            epochLoss += lossValue.item() # record loss\n",
    "\n",
    "            lossValue.backward() # backward\n",
    "            optimizer.step()\n",
    "\n",
    "        epochLoss /= len(dataLoader)\n",
    "        epochsLoss.append(epochLoss) # record loss\n",
    "\n",
    "        if Verbose and epoch % (epochs/10) == (epochs/10-1):\n",
    "            print(f\"Epoch[{epoch + 1:>{paddingLength}}] Complete: Avg. Loss: {epochLoss:.8f}\")\n",
    "    return epochsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAEModel = DAE(RBMLayers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 256 # use bigger batch size for fine tinning\n",
    "dataLoader = DataLoader(MNISTTrain, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(DAEModel.parameters(), lr=1e-3)\n",
    "loss = batchMSELoss(batchSize)\n",
    "epochsLoss = trainPytorchModel(DAEModel, optimizer, loss=loss, dataLoader=dataLoader, epochs=50)\n",
    "\n",
    "# display recored loss values\n",
    "plt.plot(epochsLoss, color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchLossReport(dataLoader, model, loss):\n",
    "    lossValues = []\n",
    "    model.eval() # Disable some specific layers/parts(Dropouts Layers, BatchNorm Layers, ...)\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataLoader:\n",
    "            inputs = inputs.to(device=device)\n",
    "            outputs = model(inputs)\n",
    "            lossValues.append(loss(inputs, outputs).item())\n",
    "    model.train()\n",
    "    return np.array(lossValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBatchLoss = batchLossReport(trainDataLoader, DAEModel, batchMSELoss(trainDataLoader.batch_size))\n",
    "testBatchLoss = batchLossReport(testDataLoader, DAEModel, batchMSELoss(testDataLoader.batch_size))\n",
    "\n",
    "plt.plot(trainBatchLoss, color='blue', label = f\"train, Avg MSE : {trainBatchLoss.mean()}\")\n",
    "plt.plot(testBatchLoss, color='red', label = f\"test, Avg MSE : {testBatchLoss.mean()}\")\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Avg batch MSE Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = getAllBatchData(testDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "DAEModel.eval()\n",
    "with torch.no_grad():\n",
    "    DAECompression = DAEModel.encode(torch.from_numpy(data).float().to(device)).detach().cpu().numpy()\n",
    "    DAEDecompression = DAEModel(torch.from_numpy(data).float().to(device)).detach().cpu().numpy()\n",
    "DAEModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAEDecompression = (DAEDecompression*255).astype(np.uint8)\n",
    "displayOutputNumpy(DAEDecompression[:80], data[:80], dim=(28, 28),\n",
    "                   title = f\"DAE Reconstruction {DAEModel.layersStr()}\",\n",
    "                   fileName = f\"./imgs/training/RBMReconstruct({DAEModel.layersStr()}).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=DAECompression[:, 0], y=DAECompression[:, 1], z=DAECompression[:, 2], text=labels,\n",
    "            mode='markers',\n",
    "           marker=dict(size=4,opacity=0.5, color=labels*2,colorscale='Turbo'), \n",
    "           name=\"data\",\n",
    "           hoverinfo='text',\n",
    "      )])\n",
    "# https://plotly.com/python/builtin-colorscales/\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data mean normalisation\n",
    "dataMean = data.mean(axis = 0)\n",
    "normalizedData = data - dataMean\n",
    "\n",
    "covMat = np.cov(normalizedData, rowvar=False)\n",
    "\n",
    "values, vectors = eigh(covMat) # finding eigen-values and corresponding eigen-vectors \n",
    "nb = 3\n",
    "\n",
    "explainedVarianceSum = np.cumsum(values[::-1] / np.sum(values))[:nb]\n",
    "principalEv = vectors[:, -nb:].T # keep only 3 components\n",
    "print(f'Explained variance of {nb} main components : {explainedVarianceSum[-1]}')\n",
    "\n",
    "projected = data @ principalEv.T\n",
    "backProjected = projected @ principalEv\n",
    "# back normalisation\n",
    "backProjected =  backProjected + dataMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayOutputNumpy(backProjected[:80], data[:80], dim=(28, 28),\n",
    "                   title = f\"PCA Reconstruction {nb} (Explained variance: {explainedVarianceSum[-1]})\",\n",
    "                   fileName = f\"./imgs/training/PCA({nb}).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=projected[:, 0], y=projected[:, 1], z=projected[:, 2], text=labels,\n",
    "            mode='markers', \n",
    "           marker=dict(size=3,opacity=0.5, color=labels*2,colorscale='Turbo'), \n",
    "           name=\"data\",\n",
    "           hoverinfo='text',\n",
    "      )])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 784-1000-500-250-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBMLayers30 = TrainsRBMLayers([784, 1000, 500, 250, 100, 30], batchMSELoss(trainDataLoader.batch_size), trainDataLoader,  savePrefixe = \"02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAEModel30 = DAE(RBMLayers30).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 256 # use bigger batch size for fine tinning\n",
    "dataLoader = DataLoader(MNISTTrain, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(DAEModel30.parameters(), lr=1e-3)\n",
    "loss = batchMSELoss(batchSize)\n",
    "epochsLoss = trainPytorchModel(DAEModel30, optimizer, loss=loss, dataLoader=dataLoader, epochs=150)\n",
    "\n",
    "# display recored loss values\n",
    "plt.plot(epochsLoss, color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = getAllBatchData(testDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "DAEModel30.eval()\n",
    "with torch.no_grad():\n",
    "    DAEDecompression30 = DAEModel30(torch.from_numpy(data).float().to(device)).detach().cpu().numpy()\n",
    "DAEModel30.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAEDecompression30 = (DAEDecompression30*255).astype(np.uint8)\n",
    "displayOutputNumpy(DAEDecompression30[:80], data[:80], dim=(28, 28),\n",
    "                   title = f\"DAE Reconstruction {DAEModel30.layersStr()}\",\n",
    "                   fileName = f\"./imgs/training/RBMReconstruct({DAEModel30.layersStr()}).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA keep only 30 components\n",
    "nb = 30\n",
    "# data mean normalisation\n",
    "dataMean = data.mean(axis = 0)\n",
    "normalizedData = data - dataMean\n",
    "\n",
    "covMat = np.cov(normalizedData, rowvar=False)\n",
    "\n",
    "values, vectors = eigh(covMat)\n",
    "explainedVarianceSum = np.cumsum(values[::-1] / np.sum(values))[:nb]\n",
    "comps = vectors[:, -nb:].T\n",
    "print(f'Explained variance of {nb} main components : {explainedVarianceSum[-1]}')\n",
    "\n",
    "projected = data @ principalEv.T\n",
    "backProjected = (data @ comps.T) @ comps\n",
    "backProjected =  backProjected + dataMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayOutputNumpy(backProjected[:80], data[:80], dim=(28, 28),\n",
    "                   title = f\"PCA Reconstruction {nb} (Explained variance: {explainedVarianceSum[-1]})\",\n",
    "                   fileName = f\"./imgs/training/PCA({nb}).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBatchLoss = batchLossReport(trainDataLoader, DAEModel30, batchMSELoss(trainDataLoader.batch_size))\n",
    "testBatchLoss = batchLossReport(testDataLoader, DAEModel30, batchMSELoss(testDataLoader.batch_size))\n",
    "\n",
    "plt.plot(trainBatchLoss, color='blue', label = f\"train, Avg MSE : {trainBatchLoss.mean()}\")\n",
    "plt.plot(testBatchLoss, color='red', label = f\"test , Avg MSE : {testBatchLoss.mean()}\")\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Avg batch MSE Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit31566a2438ca44acb4bfb2957d6499e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

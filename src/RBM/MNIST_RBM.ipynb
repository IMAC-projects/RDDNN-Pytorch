{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=(20,20)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from RBM import RBM\n",
    "from DAE import DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenTransform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "MNISTTrain = torchvision.datasets.MNIST(root=\"./dataset/\", train=True, transform=flattenTransform, download=True)\n",
    "MNISTTest = torchvision.datasets.MNIST(root=\"./dataset/\", train=False, transform=flattenTransform, download=True)\n",
    "\n",
    "batchSize = 128\n",
    "trainData = DataLoader(MNISTTrain, batch_size=batchSize, shuffle=True)\n",
    "testData = DataLoader(MNISTTest, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayOutput(a, b, dim=(28, 28), maxDisplay = 10, title=None, fileName=None):\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    for i, data in enumerate((a, b)):\n",
    "        viewAsImage = data.view(data.shape[0], 1, dim[0], dim[1])\n",
    "        # use sqrt to compute max squared size length before makeGrid\n",
    "        # sideSize = min(int(math.sqrt(len(viewAsImage))), maxDisplay)\n",
    "        # img = make_grid(viewAsImage[:sideSize*sideSize].data, nrow = sideSize).detach().cpu().numpy()\n",
    "        img = make_grid(viewAsImage.data).detach().cpu().numpy()\n",
    "        axs[i].imshow(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "    if title is not None: fig.suptitle(title)\n",
    "    if fileName is not None: plt.savefig(fileName)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRBM(rbm, dataLoader, epochs, learningRate, weightDecay=2e-4):\n",
    "    loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        trainLoss = 0\n",
    "        for i, (data, _) in enumerate(dataLoader): # we will use only data\n",
    "            Vp0 = data.to(device)\n",
    "            # Vs0 = torch.bernoulli(Vp0)\n",
    "            # V: Visible | H: Hidden\n",
    "            # s: sampling | p: probabilities\n",
    "            # 0: start | k:end\n",
    "\n",
    "            Vpk, Vsk = rbm.gibbsSampling(Vp0, iterations = 1) #Vs0\n",
    "\n",
    "            Hp0, _ = rbm.sampleHidden(Vp0) #Vs0\n",
    "            Hpk, _ = rbm.sampleHidden(Vpk) #Vsk\n",
    "\n",
    "            #Vs0\n",
    "            rbm.contrastiveDivergence(Vp0, Vpk, Hp0, Hpk, learningRate = learningRate, weightDecay = weightDecay, momentumDamping = 0.5 if epoch < 5 else 0.9)\n",
    "\n",
    "            trainLoss += loss(Vp0, Vpk) # track loss of probabilities\n",
    "\n",
    "        if epoch % (epochs/5) == (epochs/5-1):\n",
    "            print(f\"epoch[{ epoch+1:>{ int(math.log10(epochs)) } } ] Complete: Avg. Loss: {epochLoss / len(dataLoader):.6f}\")\n",
    "        \n",
    "        print(f\"epoch {epoch+1}: {trainLoss/len(dataLoader)}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genNewDataSet(rbm, device, dataLoader):\n",
    "    # rederive new data loader based on hidden activations of trained model\n",
    "    newData = []\n",
    "    for data, _ in dataLoader:\n",
    "        Hp, _ = rbm.sampleHidden(data.to(device))\n",
    "        newData.append(Hp.detach().cpu().numpy())\n",
    "    newData = np.concatenate(newData)\n",
    "    fakesLabels = np.zeros((len(newData), 1))\n",
    "    return DataLoader(TensorDataset(torch.Tensor(newData).to(device), torch.Tensor(fakesLabels).to(device)), batch_size=dataLoader.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use much lower learning for last gaussian layer to avoid exploding gradient\n",
    "# use a Gaussian distribution for the last hidden layer to let it take advantage of continuous values\n",
    "# 784-1000-500-250-3\n",
    "epochsOffset = 0\n",
    "RMBLayersTrainingInfos = [\n",
    "    { \"hiddenDim\": 1000, \"numEpochs\": 10 + epochsOffset, \"learningRate\": 0.1, \"displayDim\": (28, 28), \"useGaussian\": False}, \n",
    "    { \"hiddenDim\": 500, \"numEpochs\": 10 + epochsOffset, \"learningRate\": 0.1, \"displayDim\": (25, 40), \"useGaussian\": False},\n",
    "    { \"hiddenDim\": 250, \"numEpochs\": 10 + epochsOffset, \"learningRate\": 0.1, \"displayDim\": (25, 20), \"useGaussian\": False},\n",
    "    { \"hiddenDim\": 3, \"numEpochs\": 30 + epochsOffset, \"learningRate\": 0.01, \"displayDim\": (25, 10), \"useGaussian\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "dataLoader = trainData # get initial iteration of new training data\n",
    "visibleDim = next(iter(dataLoader))[0].shape[1] # set starting visible dim\n",
    "hiddenDim = None\n",
    "RBMLayers = [] # trained RBM models\n",
    "\n",
    "for configs in RMBLayersTrainingInfos:\n",
    "    # update hidenDim\n",
    "    hiddenDim = configs[\"hiddenDim\"]\n",
    "    # create rbm layers\n",
    "    rbm = RBM(device, visibleDim, hiddenDim, gaussianHiddenDistribution=configs[\"useGaussian\"], useMomentum = True)\n",
    "\n",
    "    # # display sample output\n",
    "    # data = next(iter(dataLoader))[0].to(device)\n",
    "    # reconstructedVp, _ = rbm.reconstruct(data)\n",
    "    # displayOutput(data, reconstructedVp, configs[\"displayDim\"], title=f'MSE: {loss(data, reconstructedVp).item()}')\n",
    "\n",
    "    trainRBM(rbm, dataLoader, configs[\"numEpochs\"], configs[\"learningRate\"], weightDecay=2e-4)\n",
    "\n",
    "    # display sample output\n",
    "    data = next(iter(dataLoader))[0].to(device)\n",
    "    reconstructedVp, _ = rbm.reconstruct(data)\n",
    "    displayOutput(data, reconstructedVp, configs[\"displayDim\"], title=f'MSE: {loss(data, reconstructedVp).item()}')\n",
    "\n",
    "    RBMLayers.append(rbm)\n",
    "    dataLoader = genNewDataSet(rbm, device, dataLoader)\n",
    "    visibleDim = hiddenDim # update new visibleDim for next RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & fine-tune autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = trainData\n",
    "learningRate = 1e-3\n",
    "DAE = DAE(RBMLayers).to(device)\n",
    "loss = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(DAE.parameters(), learningRate)\n",
    "numEpochs = 50\n",
    "\n",
    "trackLoss = True\n",
    "epochsLoss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(numEpochs):\n",
    "    epochLoss = 0\n",
    "    for batchIdx, (data, _) in enumerate(dataLoader):\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # zero the parameters gradients\n",
    "\n",
    "        outputs = DAE(data) # forward\n",
    "\n",
    "        lossValue = loss(data, outputs) # compute loss\n",
    "\n",
    "        if(trackLoss):  # record loss\n",
    "            epochLoss += lossValue.item()\n",
    "\n",
    "        lossValue.backward() # backward\n",
    "        optimizer.step()\n",
    "\n",
    "    epochLoss /= len(dataLoader)\n",
    "    if(trackLoss):  # record loss\n",
    "        epochsLoss.append(epochLoss)\n",
    "    \n",
    "    if epoch % (numEpochs/10) == (numEpochs/10-1):\n",
    "        print(f\"Epoch[{epoch + 1:>4}] Complete: Avg. Loss: {epochLoss:.8f}\")\n",
    "        # displayOutput(data, outputs, (28, 28), title=f'MSE: {loss(data, outputs).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dataLoader)\n",
    "\n",
    "for i in range(4):\n",
    "    data, _ = next(it)\n",
    "    data = data.to(device)\n",
    "    outputs = DAE(data)\n",
    "    displayOutput(data, outputs, (28, 28), title=f'MSE: {loss(data, outputs).item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

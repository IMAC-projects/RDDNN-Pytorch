{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=(20,20)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from RBM import RBM\n",
    "from DAE import DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenTransform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "MNISTTrain = torchvision.datasets.MNIST(root=\"./dataset/\", train=True, transform=flattenTransform, download=True)\n",
    "MNISTTest = torchvision.datasets.MNIST(root=\"./dataset/\", train=False, transform=flattenTransform, download=True)\n",
    "\n",
    "batchSize = 128\n",
    "trainData = DataLoader(MNISTTrain, batch_size=batchSize, shuffle=True)\n",
    "testData = DataLoader(MNISTTest, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayOutput(a, b, dim=(28, 28), maxDisplay = 10, title=None, fileName=None):\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    for i, data in enumerate((a, b)):\n",
    "        viewAsImage = data.view(data.shape[0], 1, dim[0], dim[1])\n",
    "        # use sqrt to compute max squared size length before makeGrid\n",
    "        # sideSize = min(int(math.sqrt(len(viewAsImage))), maxDisplay)\n",
    "        # img = make_grid(viewAsImage[:sideSize*sideSize].data, nrow = sideSize).detach().cpu().numpy()\n",
    "        img = make_grid(viewAsImage.data).detach().cpu().numpy()\n",
    "        axs[i].imshow(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "    if title is not None: fig.suptitle(title)\n",
    "    if fileName is not None: plt.savefig(fileName)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSELossSum = torch.nn.MSELoss(reduction='sum')\n",
    "def batchLoss(a, b, batchSize):\n",
    "    return MSELossSum(a,b)/batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRBM(rbm, dataLoader, epochs, learningRate, weightDecay=2e-4):\n",
    "    paddingLength = 1+int(math.log10(epochs)) # for padded print\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epochLoss = 0\n",
    "        for i, (data, _) in enumerate(dataLoader): # we will use only data\n",
    "            Vp0 = data.to(device)\n",
    "            # Vs0 = torch.bernoulli(Vp0)\n",
    "            # V: Visible | H: Hidden\n",
    "            # s: sampling | p: probabilities\n",
    "            # 0: start | k:end\n",
    "            \n",
    "            Vpk, Vsk = rbm.gibbsSampling(Vp0, iterations = 1) #Vs0\n",
    "              \n",
    "            Hp0, _ = rbm.sampleHidden(Vp0) #Vs0\n",
    "            Hpk, _ = rbm.sampleHidden(Vpk) #Vsk\n",
    "            \n",
    "            rbm.contrastiveDivergence(Vp0, Vpk, Hp0, Hpk, learningRate = learningRate, weightDecay = weightDecay, momentumDamping = 0.5 if epoch < 5 else 0.9)\n",
    "\n",
    "            epochLoss += batchLoss(Vp0, Vpk, batchSize)\n",
    "\n",
    "        if epoch % (epochs/10) == (epochs/10-1):\n",
    "            print(f\"  Epoch[{epoch+1:>{paddingLength}}] Avg. Loss: {epochLoss / len(dataLoader):.5f}\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genNewDataSet(rbm, device, dataLoader):\n",
    "    # rederive new data loader based on hidden activations of trained model\n",
    "    newData = []\n",
    "    for data, _ in dataLoader:\n",
    "        Hp, _ = rbm.sampleHidden(data.to(device))\n",
    "        newData.append(Hp.detach().cpu().numpy())\n",
    "    newData = np.concatenate(newData)\n",
    "    fakesLabels = np.zeros((len(newData), 1))\n",
    "    return DataLoader(TensorDataset(torch.Tensor(newData).to(device), torch.Tensor(fakesLabels).to(device)), batch_size=dataLoader.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use much lower learning for last gaussian layer to avoid exploding gradient\n",
    "# use a Gaussian distribution for the last hidden layer to let it take advantage of continuous values\n",
    "# 784-1000-500-250-3\n",
    "\n",
    "epochsF = lambda x: 2*x + 20\n",
    "RMBLayersTrainingInfos = [\n",
    "    { \"hiddenDim\": 1000, \"numEpochs\": epochsF(10), \"learningRate\":   0.1, \"displayDim\": (28, 28), \"useGaussian\": False}, \n",
    "    { \"hiddenDim\":  500, \"numEpochs\": epochsF(20), \"learningRate\":  0.05, \"displayDim\": (25, 40), \"useGaussian\": False},\n",
    "    { \"hiddenDim\":  250, \"numEpochs\": epochsF(30), \"learningRate\":  0.01, \"displayDim\": (25, 20), \"useGaussian\": False},\n",
    "    { \"hiddenDim\":    3, \"numEpochs\": epochsF(60), \"learningRate\": 0.001, \"displayDim\": (25, 10), \"useGaussian\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = trainData # get initial iteration of new training data\n",
    "visibleDim = next(iter(dataLoader))[0].shape[1] # set starting visible dim\n",
    "hiddenDim = None\n",
    "RBMLayers = [] # trained RBM models\n",
    "\n",
    "Display = True\n",
    "for configs in RMBLayersTrainingInfos:\n",
    "    hiddenDim = configs[\"hiddenDim\"]  # update hidenDim\n",
    "    numEpochs = configs[\"numEpochs\"]\n",
    "    print(f\"\\nTrain layer {visibleDim}-{hiddenDim} ({numEpochs} Epochs):\")\n",
    "    \n",
    "    # create rbm layers\n",
    "    rbm = RBM(device, visibleDim, hiddenDim, gaussianHiddenDistribution=configs[\"useGaussian\"], useMomentum = True)\n",
    "    \n",
    "    # print initial loss\n",
    "    data = next(iter(dataLoader))[0].to(device)\n",
    "    reconstructedVp, _ = rbm.reconstruct(data)\n",
    "    print(f\"  Initial first batch loss: {batchLoss(data, reconstructedVp, batchSize):.5f}\")\n",
    "    \n",
    "    trainRBM(rbm, dataLoader, numEpochs, configs[\"learningRate\"], weightDecay=2e-4)\n",
    "\n",
    "    # display sample output\n",
    "    if Display:\n",
    "        data = next(iter(dataLoader))[0].to(device)\n",
    "        reconstructedVp, _ = rbm.reconstruct(data)\n",
    "        displayOutput(data, reconstructedVp, configs[\"displayDim\"], title=f'MSE: {batchLoss(data, reconstructedVp, batchSize).item()}')\n",
    "\n",
    "    RBMLayers.append(rbm)\n",
    "    dataLoader = genNewDataSet(rbm, device, dataLoader) # generate new data based on this layer\n",
    "    visibleDim = hiddenDim # update new visibleDim for next RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & fine-tune autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 512 # use bigger batch size for fine tinning\n",
    "dataLoader = DataLoader(MNISTTrain, batch_size=batchSize, shuffle=True)\n",
    "learningRate = 1e-3\n",
    "DAE = DAE(RBMLayers).to(device)\n",
    "optimizer = optim.Adam(DAE.parameters(), learningRate)\n",
    "numEpochs = 100\n",
    "\n",
    "trackLoss = True\n",
    "epochsLoss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddingLength = 1+int(math.log10(numEpochs)) # for padded print\n",
    "for epoch in range(numEpochs):\n",
    "    epochLoss = 0\n",
    "    for batchIdx, (data, _) in enumerate(dataLoader):\n",
    "        data = data.to(device) # to device\n",
    "\n",
    "        optimizer.zero_grad() # zero the parameters gradients\n",
    "\n",
    "        outputs = DAE(data) # forward\n",
    "\n",
    "        lossValue = batchLoss(data, outputs, batchSize) # compute loss\n",
    "\n",
    "        if(trackLoss): epochLoss += lossValue.item() # record loss\n",
    "\n",
    "        lossValue.backward() # backward\n",
    "        optimizer.step()\n",
    "\n",
    "    epochLoss /= len(dataLoader)\n",
    "    if(trackLoss): epochsLoss.append(epochLoss) # record loss\n",
    "    \n",
    "    if epoch % (numEpochs/10) == (numEpochs/10-1):\n",
    "        print(f\"Epoch[{epoch + 1:>{paddingLength}}] Complete: Avg. Loss: {epochLoss:.8f}\")\n",
    "\n",
    "if trackLoss: # display recored loss values\n",
    "    plt.plot(epochsLoss, color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchLossReport(dataLoader, model):\n",
    "    lossValues = []\n",
    "    model.eval() # Disable some specific layers/parts(Dropouts Layers, BatchNorm Layers, ...)\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataLoader:\n",
    "            inputs = inputs.to(device=device)\n",
    "            outputs = model(inputs)\n",
    "            lossValues.append(batchLoss(inputs, outputs, dataLoader.batch_size))\n",
    "    model.train()\n",
    "    return lossValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBatchLoss = batchLossReport(trainData, DAE)\n",
    "testBatchLoss = batchLossReport(testData, DAE)\n",
    "\n",
    "plt.plot(trainBatchLoss, color='blue', label = \"train\")\n",
    "plt.plot(testBatchLoss, color='red', label = \"test\")\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Avg MSE Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(testData)\n",
    "\n",
    "for i in range(4):\n",
    "    data, _ = next(it)\n",
    "    data = data.to(device)\n",
    "    outputs = DAE(data)\n",
    "    displayOutput(data, outputs, (28, 28), title=f'MSE: {batchLoss(data, outputs, batchSize)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
